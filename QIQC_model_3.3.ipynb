{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"QIQC_model_3.3.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1544528481001,"user_tz":-120,"elapsed":31377,"user":{"displayName":"Quentin Legrand","photoUrl":"https://lh6.googleusercontent.com/-IXHiOGs04sE/AAAAAAAAAAI/AAAAAAAAAFk/ppsEvqzCcGw/s64/photo.jpg","userId":"00421493000358847389"}},"id":"ODRJ7bX8vV9M","outputId":"ac1b6402-c196-469f-bb26-935e74fa6b56","colab":{"base_uri":"https://localhost:8080/","height":129}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","id":"CYb4xYXtV75L","colab":{}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import os\n","from datetime import datetime as dt\n","import time\n","import re\n","%matplotlib inline\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","DRIVE_PATH = \"/content/gdrive/My Drive/Colab Notebooks/\"\n","TRAIN_FILE = \"datasets/train.csv\"\n","CORRECTED_TRAIN_FILE = \"datasets/corrected_train.csv\"\n","TEST_FILE = \"datasets/test.csv\"\n","GLOVE_FILE = \"embeddings/glove.840B.300d/glove.840B.300d.txt\""],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1544528483383,"user_tz":-120,"elapsed":33708,"user":{"displayName":"Quentin Legrand","photoUrl":"https://lh6.googleusercontent.com/-IXHiOGs04sE/AAAAAAAAAAI/AAAAAAAAAFk/ppsEvqzCcGw/s64/photo.jpg","userId":"00421493000358847389"}},"id":"_HRVliA5WoLj","outputId":"4a9d27d3-c040-4f95-f111-5e906b69cbb9","colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"8PHq3bJ-V75f"},"cell_type":"markdown","source":["# Toolbox"]},{"metadata":{"colab_type":"code","id":"xIBMtV8dV75k","colab":{}},"cell_type":"code","source":["def reset_graph(seed=42):\n","    tf.reset_default_graph()\n","    tf.set_random_seed(seed)\n","    np.random.seed(seed)\n","\n","def load_data(path = DRIVE_PATH, file = CORRECTED_TRAIN_FILE):\n","    csv_path = os.path.join(path,file)\n","    return pd.read_csv(csv_path)\n","\n","def tokenize(sentences_list):\n","    return [re.findall(r\"[\\w]+|[']|[.,!?;]\", str(x)) for x in sentences_list] \n","    \n","def get_vocab(sentences):\n","    vocab={}\n","    for sentence in sentences:\n","        for word in sentence:\n","            try:\n","                vocab[word] +=1\n","            except KeyError:\n","                vocab[word] = 1\n","    return vocab\n","\n","def glove_embeddings(vocabulary_in_set, drive = DRIVE_PATH, gloveFile = GLOVE_FILE ,extract = -1):\n","\n","    glove_file = os.path.join(drive,gloveFile)\n","  \n","    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n","\n","    embeddings = []\n","    words_id = {}\n","    f = open(glove_file,'r', encoding=\"utf8\")\n","    increment = 0\n","    words_id[\"\"]=0\n","    first = True\n","    i = 1\n","    \n","    for line in f:\n","        word, vect = get_coefs(*line.split(\" \"))\n","        if first:\n","            embeddings.append(np.zeros_like(vect))\n","            first = False\n","        if word in vocabulary_in_set:\n","            embeddings.append(vect)\n","            words_id[word] = i\n","            i += 1\n","            if increment == extract - 1:\n","                break\n","            elif extract != -1:\n","                increment += 1\n","    f.close()   \n","    return np.array(embeddings), words_id\n","\n","def df_to_data_target(df, data_col = \"corrected_question_text\", target_col = \"target\"):\n","    data = df[data_col]\n","    target = df[target_col]\n","    return np.c_[data, target]\n","\n","def embed_and_pad(X, embeddings, n_dims, width):\n","    padded_X = np.zeros((len(X),width,n_dims))\n","    i = 0\n","    for sentence in X:\n","        j = 0\n","        for word in sentence:\n","            padded_X[i,j,:] = embeddings[word]\n","            j += 1\n","        i +=1\n","    return padded_X"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"xPTqJyieV75u","colab":{}},"cell_type":"code","source":["class data_split_and_batch:\n","    def __init__(self, data_targets, dv=0, cv=0, batch_size=500, words_ids = None, seed=42):\n","        self.seed = seed\n","        np.random.seed(seed)\n","        self.batch_length = batch_size\n","        self.pointer = 0\n","        self.reshuffle_seed = 0\n","        self.__split__(data_targets=data_targets, dv=dv, cv=cv, words_ids=words_ids)\n","    \n","    def __split__(self, data_targets, dv, cv, words_ids):\n","        positive_data_targets = data_targets[data_targets[:,1]==1]\n","        negative_data_targets = data_targets[data_targets[:,1]==0]\n","        \n","        positive_length = len(positive_data_targets)\n","        negative_length = len(negative_data_targets)\n","  \n","        pos_data_targets = positive_data_targets[np.random.permutation(positive_length), :]\n","        neg_data_targets = negative_data_targets[np.random.permutation(negative_length), :]\n","        \n","        if dv != 0:\n","            dv_positive = int(dv * positive_length/(positive_length + negative_length))\n","            dv_negative = dv - dv_positive\n","            dv_data_targets = np.concatenate((positive_data_targets[0:dv_positive,:], negative_data_targets[0:dv_negative,:]))\n","            dv_data_targets = dv_data_targets[np.random.permutation(dv), :]\n","            self.dv_targets = dv_data_targets[:,1].astype(float)\n","            X_dev = self.__tokenize__(dv_data_targets[:,0])\n","            self.X_dv_lengths = np.array(list(map(len, X_dev)))\n","            if words_ids == None:\n","                self.dv_data = X_dev\n","            else:\n","                self.dv_data = self.__ids_and_pad__(X_dev,words_ids[0], words_ids[1])\n","        else:\n","            dv_positive = 0\n","            dv_negative = 0\n","            \n","        if cv != 0:\n","            cv_positive = int(cv * positive_length/(positive_length + negative_length))\n","            cv_negative = cv - cv_positive\n","            cv_data_targets = np.concatenate((positive_data_targets[dv_positive:dv_positive + cv_positive,:], negative_data_targets[dv_negative:dv_negative + cv_negative,:]))\n","            cv_data_targets = cv_data_targets[np.random.permutation(cv), :]\n","            self.cv_targets = cv_data_targets[:,1].astype(float)\n","            X_cross = self.__tokenize__(cv_data_targets[:,0])\n","            self.X_cv_lengths = np.array(list(map(len, X_cross)))\n","            if words_ids == None:\n","                self.cv_data = X_cross\n","            else:\n","                self.cv_data = self.__ids_and_pad__(X_cross,words_ids[0], words_ids[1])   \n","        else:\n","            cv_positive = 0\n","            cv_negative = 0\n","            \n","        train_data_targets = np.concatenate((positive_data_targets[dv_positive + cv_positive:,:], negative_data_targets[dv_negative + cv_negative:,:]))\n","        train_data_targets = train_data_targets[np.random.permutation(len(data_targets)-dv-cv), :]\n","        self.train_targets = train_data_targets[:,1].astype(float)\n","        X_train = self.__tokenize__(train_data_targets[:,0])\n","        self.X_train_lengths = np.array(list(map(len, X_train)))\n","        if words_ids == None:\n","            self.train_data = X_train\n","        else:\n","            self.train_data = self.__ids_and_pad__(X_train ,words_ids[0], words_ids[1])\n","        self.num_train_examples = len(self.train_targets)\n","        self.nr_batches = self.num_train_examples//self.batch_length\n","    \n","    def __tokenize__(self, X):\n","        return [re.findall(r\"[\\w]+|[']|[.,!?;]\", str(x)) for x in X]\n","    \n","    def __ids_and_pad__(self, X, word_ids, width):\n","        padded_X = np.zeros((len(X),width))\n","        i = 0\n","        for sentence in X:\n","            j = 0\n","            for word in sentence:\n","                padded_X[i,j] = word_ids[word]\n","                j += 1\n","            i +=1\n","        return padded_X\n","    \n","    def cross_val_set(self):\n","        return self.X_cv_lengths, self.cv_data, self.cv_targets \n","    \n","    def dev_set(self):\n","        return self.X_dv_lengths, self.dv_data, self.dv_targets\n","    \n","    def next_batch(self):\n","        self.pointer += 1\n","        X_batch = self.train_data[(self.pointer - 1)*self.batch_length:self.pointer*self.batch_length]\n","        y_batch = self.train_targets[(self.pointer - 1)*self.batch_length:self.pointer*self.batch_length]\n","        X_batch_lengths = self.X_train_lengths[(self.pointer - 1)*self.batch_length:self.pointer*self.batch_length]\n","        \n","        if self.pointer == self.nr_batches:\n","            self.pointer = 0\n","            self.reshuffle_seed += 1\n","            np.random.seed(self.seed + self.reshuffle_seed)\n","            permutations = np.random.permutation(self.num_train_examples)\n","            self.train_data = self.train_data[permutations]\n","            self.train_targets = self.train_targets[permutations]\n","            self.X_train_lengths = self.X_train_lengths[permutations]\n","            \n","        return X_batch_lengths, X_batch, y_batch\n","    \n","    def reset(self):\n","        self.pointer = 0        "],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"8OJMLwdPV757"},"cell_type":"markdown","source":["# Load and prepare data"]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1544528489698,"user_tz":-120,"elapsed":39978,"user":{"displayName":"Quentin Legrand","photoUrl":"https://lh6.googleusercontent.com/-IXHiOGs04sE/AAAAAAAAAAI/AAAAAAAAAFk/ppsEvqzCcGw/s64/photo.jpg","userId":"00421493000358847389"}},"id":"06xzg79TV75-","outputId":"82b22694-56e2-4171-f29b-b47c103a66fb","colab":{"base_uri":"https://localhost:8080/","height":206}},"cell_type":"code","source":["train_data = load_data()\n","train_data.head()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>qid</th>\n","      <th>target</th>\n","      <th>corrected_question_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>00002165364db923c7e6</td>\n","      <td>0</td>\n","      <td>How did Quebec nationalists see their province...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000032939017120e6e44</td>\n","      <td>0</td>\n","      <td>Do you have an adopted dog , how would you enc...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0000412ca6e4628ce2cf</td>\n","      <td>0</td>\n","      <td>Why does velocity affect time ? Does velocity ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>000042bf85aa498cd78e</td>\n","      <td>0</td>\n","      <td>How did Otto von Guericke used the Magdeburg h...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0000455dfa3e01eae3af</td>\n","      <td>0</td>\n","      <td>Can I convert montra helicon D to a mountain b...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                    qid  target  \\\n","0  00002165364db923c7e6       0   \n","1  000032939017120e6e44       0   \n","2  0000412ca6e4628ce2cf       0   \n","3  000042bf85aa498cd78e       0   \n","4  0000455dfa3e01eae3af       0   \n","\n","                             corrected_question_text  \n","0  How did Quebec nationalists see their province...  \n","1  Do you have an adopted dog , how would you enc...  \n","2  Why does velocity affect time ? Does velocity ...  \n","3  How did Otto von Guericke used the Magdeburg h...  \n","4  Can I convert montra helicon D to a mountain b...  "]},"metadata":{"tags":[]},"execution_count":6}]},{"metadata":{"colab_type":"code","id":"TFTmNHrfV76L","colab":{}},"cell_type":"code","source":["data_targets = df_to_data_target(train_data)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"s8-E1GHuV76V","colab":{}},"cell_type":"code","source":["tokenized_questions = tokenize(train_data[\"corrected_question_text\"].values)\n","vocabulary_in_set = get_vocab(tokenized_questions)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"31VolUA5V76l","colab":{}},"cell_type":"code","source":["embeddings, words_ids = glove_embeddings(vocabulary_in_set=vocabulary_in_set)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"pOVK2aYgV77U"},"cell_type":"markdown","source":["# Construction phase"]},{"metadata":{"colab_type":"code","id":"er4v1HWSV77a","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_neurons = [900, 600, 300, 200]\n","n_fc1 = 1500\n","n_fc2 = 600\n","n_fc3 = 200\n","n_output = 1\n","learning_rate = tf.placeholder(tf.float32)\n","activation_fn = tf.nn.elu\n","\n","n_steps = 250\n","seq_len = tf.placeholder(tf.int32, [None], name=\"seq_len\")\n","\n","X = tf.placeholder(tf.int32, [None, n_steps], name=\"X\")\n","y = tf.placeholder(tf.float32, [None], name=\"y\")\n","\n","with tf.name_scope(\"embedding_layer\"):\n","    word_embeddings = tf.Variable(initial_value=embeddings, trainable=False)\n","    embedded_sentences = tf.nn.embedding_lookup(word_embeddings, X)\n","\n","with tf.name_scope(\"shallow_LSTM_layer\"):\n","    lstm_cell = [tf.contrib.rnn.LSTMCell(num_units=n, name=\"LSTM\") for n in n_neurons]\n","    multi_lstm_cell = tf.contrib.rnn.MultiRNNCell(lstm_cell)\n","    h_states, out_states = tf.nn.dynamic_rnn(multi_lstm_cell, embedded_sentences, dtype = tf.float32, sequence_length=seq_len) #state = 3 * 2 * m * 500\n","    states = tf.concat([out_states[0][0],out_states[1][0],out_states[2][0], out_states[3][0]], axis = 1)\n","    \n","with tf.name_scope(\"fully_connected_layers\"):\n","    fc1 = tf.contrib.layers.fully_connected(states, n_fc1, activation_fn=activation_fn)\n","    fc2 = tf.contrib.layers.fully_connected(fc1, n_fc2, activation_fn=activation_fn)\n","    fc3 = tf.contrib.layers.fully_connected(fc2, n_fc3, activation_fn=activation_fn)\n","\n","with tf.name_scope(\"logits_and_outputs\"):\n","    logits = tf.reshape(tf.contrib.layers.fully_connected(fc3, n_output, activation_fn=None),shape=[-1])\n","    outputs = tf.sigmoid(logits)\n","    predictions = tf.round(outputs)\n","\n","with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy)\n","\n","with tf.name_scope(\"evaluation\"):\n","    TP = tf.count_nonzero(predictions * y)\n","    TN = tf.count_nonzero((predictions - 1) * (y - 1))\n","    FP = tf.count_nonzero(predictions * (y - 1))\n","    FN = tf.count_nonzero((predictions - 1) * y)\n","    precision = TP / (TP + FP)\n","    recall = TP / (TP + FN)\n","    accuracy = (TP + TN)/(TP + FP + TN + FN)\n","    f1 = 2 * precision * recall / (precision + recall)\n","\n","with tf.name_scope(\"training_op\"):\n","    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","    training_op = optimizer.minimize(loss)\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()\n","\n","f1_train_summary = tf.summary.scalar('Train_F1_score', f1)\n","f1_dev_summary = tf.summary.scalar(\"Dev_F1_score\", f1)\n","loss_train_summary = tf.summary.scalar(\"Train_loss\", loss)\n","loss_dev_summary = tf.summary.scalar(\"Dev_loss\", loss)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"-66y9w6WV77g"},"cell_type":"markdown","source":["# Training phase"]},{"metadata":{"colab_type":"code","id":"MayLFImnV77p","colab":{}},"cell_type":"code","source":["batch_size = 500\n","\n","QIQC = data_split_and_batch(data_targets, cv=1000, dv=1000, batch_size=batch_size, words_ids=(words_ids,n_steps))\n","X_dev_lengths, X_dev_set, y_dev_set = QIQC.dev_set()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Euh-9DfDZMRj","colab_type":"code","outputId":"d705c5e2-ccd6-4590-8559-1c5043a03940","executionInfo":{"status":"ok","timestamp":1544528772082,"user_tz":-120,"elapsed":322308,"user":{"displayName":"Quentin Legrand","photoUrl":"https://lh6.googleusercontent.com/-IXHiOGs04sE/AAAAAAAAAAI/AAAAAAAAAFk/ppsEvqzCcGw/s64/photo.jpg","userId":"00421493000358847389"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"cell_type":"code","source":["\"\"\"\n","QIQC.reset()\n","with tf.Session() as sess:\n","    init.run()\n","    loss_hist = []\n","    lr_hist = []\n","    iter_hist = []\n","    lr = 0\n","    t_0 = time.time()\n","    for iteration in range(QIQC.nr_batches):  \n","        lr = (iteration + 1)/QIQC.nr_batches * 10 # 1.008188^85 = 2\n","        X_lengths, X_batch, y_batch = QIQC.next_batch()\n","        _, run_loss = sess.run([training_op, loss], feed_dict={X: X_batch, y: y_batch, seq_len: X_lengths, learning_rate:lr})\n","\n","        loss_hist.append(run_loss)\n","        iter_hist.append(iteration)\n","        lr_hist.append(lr)\n","        if iteration % 100 == 0:\n","            t_1 = time.time()\n","            t_delta = t_1 - t_0\n","            expected_runtime = t_delta * (QIQC.nr_batches - iteration)/100\n","            expected_runtime_h = int(expected_runtime//3600)\n","            expected_runtime_min = int((expected_runtime - 3600 * expected_runtime_h)//60)\n","            expected_runtime_sec = int((expected_runtime - 3600 * expected_runtime_h - 60 * expected_runtime_min))\n","            t_0 = t_1\n","            print(\"It. {}/{}; Runtime {:.2f} s; Remaining runtime {}h{}min{}s\".format(iteration, QIQC.nr_batches, t_delta, expected_runtime_h,expected_runtime_min,expected_runtime_sec))\n","    \n","\"\"\"     "],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nQIQC.reset()\\nwith tf.Session() as sess:\\n    init.run()\\n    loss_hist = []\\n    lr_hist = []\\n    iter_hist = []\\n    lr = 0\\n    t_0 = time.time()\\n    for iteration in range(QIQC.nr_batches):  \\n        lr = (iteration + 1)/QIQC.nr_batches * 10 # 1.008188^85 = 2\\n        X_lengths, X_batch, y_batch = QIQC.next_batch()\\n        _, run_loss = sess.run([training_op, loss], feed_dict={X: X_batch, y: y_batch, seq_len: X_lengths, learning_rate:lr})\\n\\n        loss_hist.append(run_loss)\\n        iter_hist.append(iteration)\\n        lr_hist.append(lr)\\n        if iteration % 100 == 0:\\n            t_1 = time.time()\\n            t_delta = t_1 - t_0\\n            expected_runtime = t_delta * (QIQC.nr_batches - iteration)/100\\n            expected_runtime_h = int(expected_runtime//3600)\\n            expected_runtime_min = int((expected_runtime - 3600 * expected_runtime_h)//60)\\n            expected_runtime_sec = int((expected_runtime - 3600 * expected_runtime_h - 60 * expected_runtime_min))\\n            t_0 = t_1\\n            print(\"It. {}/{}; Runtime {:.2f} s; Remaining runtime {}h{}min{}s\".format(iteration, QIQC.nr_batches, t_delta, expected_runtime_h,expected_runtime_min,expected_runtime_sec))\\n    \\n'"]},"metadata":{"tags":[]},"execution_count":12}]},{"metadata":{"id":"NodUNAzSqAuk","colab_type":"code","colab":{}},"cell_type":"code","source":["#plt.plot(np.log(lr_hist), loss_hist)\n","#plt.ylim((0,30))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","executionInfo":{"status":"error","timestamp":1544536314373,"user_tz":-120,"elapsed":41254,"user":{"displayName":"Quentin Legrand","photoUrl":"https://lh6.googleusercontent.com/-IXHiOGs04sE/AAAAAAAAAAI/AAAAAAAAAFk/ppsEvqzCcGw/s64/photo.jpg","userId":"00421493000358847389"}},"id":"U6YJBLgtV775","outputId":"7b7e87bc-14ee-49b5-f067-d1cabe375f80","colab":{"base_uri":"https://localhost:8080/","height":2323}},"cell_type":"code","source":["QIQC.reset()\n","n_epoch = 5\n","\n","now = dt.utcnow().strftime(\"%Y%m%d%H%M%S\")\n","LOGS_FOLDER = \"logs/run-{}/\".format(now)\n","MODELS_FILE = \"models/model3.3/QIQC_model3.3.ckpt\"\n","logdir = os.path.join(DRIVE_PATH,LOGS_FOLDER)\n","modfil = os.path.join(DRIVE_PATH,MODELS_FILE)\n","\n","file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n","total_deltas = 0\n","t_init = time.time()\n","min_lr = 0.0005\n","max_lr = 0.0015\n","with tf.Session() as sess:\n","    init.run()\n","    t_0 = time.time()\n","    stepsize = QIQC.nr_batches\n","    for epoch in range(n_epoch):\n","        for iteration in range(QIQC.nr_batches):\n","            step = epoch * QIQC.nr_batches + iteration\n","            cycle = np.floor(1 + step/(2 * stepsize))\n","            pattern = np.absolute(step/stepsize - 2*cycle + 1)\n","            cycle_lr = min_lr + (max_lr - min_lr) * pattern\n","            X_lengths, X_batch, y_batch = QIQC.next_batch()\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, seq_len: X_lengths, learning_rate: cycle_lr})\n","\n","            if iteration % 30 == 0 :\n","                f1_train_str, loss_train_str = sess.run([f1_train_summary, loss_train_summary],feed_dict = {X : X_batch, y : y_batch, seq_len: X_lengths})\n","                f1_dev_str, loss_dev_str = sess.run([f1_dev_summary, loss_dev_summary], feed_dict = {X : X_dev_set, y : y_dev_set, seq_len: X_dev_lengths})\n","                file_writer.add_summary(f1_train_str, step)\n","                file_writer.add_summary(f1_dev_str, step)\n","                file_writer.add_summary(loss_train_str, step)\n","                file_writer.add_summary(loss_dev_str, step)\n","\n","            if iteration % 1000 == 0:\n","                save_path = saver.save(sess,modfil)\n","            if iteration % 100 == 0:\n","                f1_train, loss_train = sess.run([f1, loss],feed_dict = {X : X_batch, y : y_batch, seq_len: X_lengths})\n","                f1_dev, loss_dev = sess.run([f1, loss], feed_dict = {X : X_dev_set, y : y_dev_set, seq_len: X_dev_lengths})\n","                t_1 = time.time()\n","                t_delta = t_1 - t_0\n","                total_deltas += t_delta\n","                expected_runtime = t_delta * ((n_epoch - epoch - 1) * QIQC.nr_batches + (QIQC.nr_batches - iteration))/100\n","                expected_runtime_h = int(expected_runtime//3600)\n","                expected_runtime_min = int((expected_runtime - 3600 * expected_runtime_h)//60)\n","                expected_runtime_sec = int((expected_runtime - 3600 * expected_runtime_h - 60 * expected_runtime_min))\n","                t_0 = t_1\n","                print(\"Ep. {}/{}; It. {}/{}; Runtime {:.2f} s; Remaining runtime {}h{}min{}s; f1_train {:.4f}; f1_dev {:.4f}; loss_train {:.4f}; loss_dev {:.4f}\".format(epoch + 1, n_epoch, iteration, QIQC.nr_batches, t_delta, expected_runtime_h,expected_runtime_min,expected_runtime_sec,f1_train,f1_dev,loss_train,loss_dev))\n","    save_path = saver.save(sess,modfil)\n","t_final = time.time()\n","actual_runtime_h = int((t_final - t_init)//3600)\n","actual_runtime_min = int((t_final - t_init - 3600 * actual_runtime_h)//60)\n","actual_runtime_sec = int((t_final - t_init - 3600 * actual_runtime_h - 60 * actual_runtime_min))\n","\n","print(\"Total actual runtime: {} hours {} min {} sec\".format(actual_runtime_h, actual_runtime_min, actual_runtime_sec))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Ep. 1/5; It. 0/2608; Runtime 9.53 s; Remaining runtime 0h20min42s; f1_train nan; f1_dev nan; loss_train 1.6409; loss_dev 1.4110\n","Ep. 1/5; It. 100/2608; Runtime 112.30 s; Remaining runtime 4h2min11s; f1_train 0.4314; f1_dev 0.5203; loss_train 0.1163; loss_dev 0.1386\n","Ep. 1/5; It. 200/2608; Runtime 109.74 s; Remaining runtime 3h54min51s; f1_train 0.4878; f1_dev 0.4222; loss_train 0.1297; loss_dev 0.1227\n","Ep. 1/5; It. 300/2608; Runtime 110.39 s; Remaining runtime 3h54min23s; f1_train 0.1379; f1_dev 0.1493; loss_train 0.1115; loss_dev 0.1426\n","Ep. 1/5; It. 400/2608; Runtime 110.53 s; Remaining runtime 3h52min50s; f1_train 0.6087; f1_dev 0.5946; loss_train 0.1548; loss_dev 0.1190\n","Ep. 1/5; It. 500/2608; Runtime 109.64 s; Remaining runtime 3h49min9s; f1_train 0.3871; f1_dev 0.3590; loss_train 0.1093; loss_dev 0.1219\n","Ep. 1/5; It. 600/2608; Runtime 109.87 s; Remaining runtime 3h47min47s; f1_train 0.5823; f1_dev 0.5962; loss_train 0.1653; loss_dev 0.1124\n","Ep. 1/5; It. 700/2608; Runtime 114.69 s; Remaining runtime 3h55min53s; f1_train 0.5507; f1_dev 0.6071; loss_train 0.1384; loss_dev 0.1151\n","Ep. 1/5; It. 800/2608; Runtime 110.58 s; Remaining runtime 3h45min34s; f1_train 0.7347; f1_dev 0.6139; loss_train 0.0819; loss_dev 0.1086\n","Ep. 1/5; It. 900/2608; Runtime 112.73 s; Remaining runtime 3h48min4s; f1_train 0.6000; f1_dev 0.6250; loss_train 0.1079; loss_dev 0.1084\n","Ep. 1/5; It. 1000/2608; Runtime 114.24 s; Remaining runtime 3h49min14s; f1_train 0.6545; f1_dev 0.5905; loss_train 0.1031; loss_dev 0.1073\n","Ep. 1/5; It. 1100/2608; Runtime 110.73 s; Remaining runtime 3h40min21s; f1_train 0.6667; f1_dev 0.5882; loss_train 0.0902; loss_dev 0.1056\n","Ep. 1/5; It. 1200/2608; Runtime 112.32 s; Remaining runtime 3h41min38s; f1_train 0.6250; f1_dev 0.6000; loss_train 0.0939; loss_dev 0.1014\n","Ep. 1/5; It. 1300/2608; Runtime 110.67 s; Remaining runtime 3h36min32s; f1_train 0.6250; f1_dev 0.5217; loss_train 0.1143; loss_dev 0.1064\n","Ep. 1/5; It. 1400/2608; Runtime 109.35 s; Remaining runtime 3h32min7s; f1_train 0.6250; f1_dev 0.6000; loss_train 0.0893; loss_dev 0.1004\n","Ep. 1/5; It. 1500/2608; Runtime 111.30 s; Remaining runtime 3h34min3s; f1_train 0.5660; f1_dev 0.6286; loss_train 0.1035; loss_dev 0.1031\n","Ep. 1/5; It. 1600/2608; Runtime 110.77 s; Remaining runtime 3h31min11s; f1_train 0.8421; f1_dev 0.6364; loss_train 0.0783; loss_dev 0.1008\n","Ep. 1/5; It. 1700/2608; Runtime 110.10 s; Remaining runtime 3h28min5s; f1_train 0.6250; f1_dev 0.5745; loss_train 0.1033; loss_dev 0.1010\n","Ep. 1/5; It. 1800/2608; Runtime 111.09 s; Remaining runtime 3h28min6s; f1_train 0.7297; f1_dev 0.6346; loss_train 0.1102; loss_dev 0.0999\n","Ep. 1/5; It. 1900/2608; Runtime 109.63 s; Remaining runtime 3h23min32s; f1_train 0.6765; f1_dev 0.6552; loss_train 0.1223; loss_dev 0.1051\n","Ep. 1/5; It. 2000/2608; Runtime 112.92 s; Remaining runtime 3h27min46s; f1_train 0.6429; f1_dev 0.6486; loss_train 0.1020; loss_dev 0.0995\n","Ep. 1/5; It. 2100/2608; Runtime 112.34 s; Remaining runtime 3h24min49s; f1_train 0.4906; f1_dev 0.5806; loss_train 0.1173; loss_dev 0.1033\n","Ep. 1/5; It. 2200/2608; Runtime 114.26 s; Remaining runtime 3h26min25s; f1_train 0.5128; f1_dev 0.6275; loss_train 0.0891; loss_dev 0.0949\n","Ep. 1/5; It. 2300/2608; Runtime 110.25 s; Remaining runtime 3h17min20s; f1_train 0.7536; f1_dev 0.6504; loss_train 0.0991; loss_dev 0.1020\n","Ep. 1/5; It. 2400/2608; Runtime 111.92 s; Remaining runtime 3h18min28s; f1_train 0.6102; f1_dev 0.6239; loss_train 0.0988; loss_dev 0.0992\n","Ep. 1/5; It. 2500/2608; Runtime 110.71 s; Remaining runtime 3h14min29s; f1_train 0.6400; f1_dev 0.5905; loss_train 0.0978; loss_dev 0.0981\n","Ep. 1/5; It. 2600/2608; Runtime 110.62 s; Remaining runtime 3h12min28s; f1_train 0.6452; f1_dev 0.6042; loss_train 0.1045; loss_dev 0.1028\n","Ep. 2/5; It. 0/2608; Runtime 18.68 s; Remaining runtime 0h32min29s; f1_train 0.7273; f1_dev 0.6019; loss_train 0.0832; loss_dev 0.0999\n","Ep. 2/5; It. 100/2608; Runtime 111.00 s; Remaining runtime 3h11min8s; f1_train 0.6364; f1_dev 0.6226; loss_train 0.0818; loss_dev 0.0972\n","Ep. 2/5; It. 200/2608; Runtime 111.57 s; Remaining runtime 3h10min15s; f1_train 0.7246; f1_dev 0.6139; loss_train 0.0947; loss_dev 0.1006\n","Ep. 2/5; It. 300/2608; Runtime 112.64 s; Remaining runtime 3h10min12s; f1_train 0.4444; f1_dev 0.5743; loss_train 0.0923; loss_dev 0.0994\n","Ep. 2/5; It. 400/2608; Runtime 111.72 s; Remaining runtime 3h6min48s; f1_train 0.6909; f1_dev 0.6355; loss_train 0.0994; loss_dev 0.0989\n","Ep. 2/5; It. 500/2608; Runtime 110.91 s; Remaining runtime 3h3min35s; f1_train 0.7500; f1_dev 0.6296; loss_train 0.1102; loss_dev 0.0982\n","Ep. 2/5; It. 600/2608; Runtime 112.04 s; Remaining runtime 3h3min35s; f1_train 0.6275; f1_dev 0.5714; loss_train 0.0944; loss_dev 0.1017\n","Ep. 2/5; It. 700/2608; Runtime 110.15 s; Remaining runtime 2h58min40s; f1_train 0.6415; f1_dev 0.6239; loss_train 0.0841; loss_dev 0.0974\n","Ep. 2/5; It. 800/2608; Runtime 110.96 s; Remaining runtime 2h58min8s; f1_train 0.5000; f1_dev 0.5806; loss_train 0.1316; loss_dev 0.1064\n","Ep. 2/5; It. 900/2608; Runtime 113.55 s; Remaining runtime 3h0min23s; f1_train 0.7324; f1_dev 0.6486; loss_train 0.1059; loss_dev 0.1008\n","Ep. 2/5; It. 1000/2608; Runtime 113.85 s; Remaining runtime 2h58min58s; f1_train 0.5714; f1_dev 0.5714; loss_train 0.1238; loss_dev 0.1016\n","Ep. 2/5; It. 1100/2608; Runtime 108.57 s; Remaining runtime 2h48min52s; f1_train 0.8108; f1_dev 0.6306; loss_train 0.0785; loss_dev 0.0967\n","Ep. 2/5; It. 1200/2608; Runtime 110.31 s; Remaining runtime 2h49min44s; f1_train 0.6207; f1_dev 0.6239; loss_train 0.1100; loss_dev 0.0980\n","Ep. 2/5; It. 1300/2608; Runtime 110.15 s; Remaining runtime 2h47min38s; f1_train 0.6667; f1_dev 0.6061; loss_train 0.0787; loss_dev 0.0964\n","Ep. 2/5; It. 1400/2608; Runtime 109.86 s; Remaining runtime 2h45min22s; f1_train 0.6774; f1_dev 0.6139; loss_train 0.0916; loss_dev 0.0968\n","Ep. 2/5; It. 1500/2608; Runtime 109.54 s; Remaining runtime 2h43min3s; f1_train 0.5152; f1_dev 0.6604; loss_train 0.1589; loss_dev 0.0962\n","Ep. 2/5; It. 1600/2608; Runtime 112.11 s; Remaining runtime 2h45min1s; f1_train 0.6933; f1_dev 0.6606; loss_train 0.1095; loss_dev 0.0957\n","Ep. 2/5; It. 1700/2608; Runtime 111.91 s; Remaining runtime 2h42min52s; f1_train 0.7037; f1_dev 0.6286; loss_train 0.0825; loss_dev 0.0966\n","Ep. 2/5; It. 1800/2608; Runtime 110.64 s; Remaining runtime 2h39min10s; f1_train 0.6154; f1_dev 0.6250; loss_train 0.1078; loss_dev 0.0950\n","Ep. 2/5; It. 1900/2608; Runtime 112.90 s; Remaining runtime 2h40min32s; f1_train 0.6207; f1_dev 0.5591; loss_train 0.0966; loss_dev 0.1047\n","Ep. 2/5; It. 2000/2608; Runtime 114.96 s; Remaining runtime 2h41min33s; f1_train 0.7241; f1_dev 0.6491; loss_train 0.0857; loss_dev 0.0931\n","Ep. 2/5; It. 2100/2608; Runtime 109.90 s; Remaining runtime 2h32min36s; f1_train 0.5833; f1_dev 0.5833; loss_train 0.1012; loss_dev 0.1050\n","Ep. 2/5; It. 2200/2608; Runtime 112.05 s; Remaining runtime 2h33min44s; f1_train 0.6377; f1_dev 0.6346; loss_train 0.1175; loss_dev 0.0935\n","Ep. 2/5; It. 2300/2608; Runtime 109.36 s; Remaining runtime 2h28min13s; f1_train 0.5625; f1_dev 0.6182; loss_train 0.1410; loss_dev 0.0971\n","Ep. 2/5; It. 2400/2608; Runtime 112.46 s; Remaining runtime 2h30min32s; f1_train 0.4878; f1_dev 0.6095; loss_train 0.0965; loss_dev 0.1049\n","Ep. 2/5; It. 2500/2608; Runtime 110.71 s; Remaining runtime 2h26min21s; f1_train 0.6222; f1_dev 0.5745; loss_train 0.0749; loss_dev 0.1014\n","Ep. 2/5; It. 2600/2608; Runtime 109.35 s; Remaining runtime 2h22min43s; f1_train 0.5085; f1_dev 0.5625; loss_train 0.1332; loss_dev 0.1026\n","Ep. 3/5; It. 0/2608; Runtime 18.41 s; Remaining runtime 0h24min0s; f1_train 0.6774; f1_dev 0.5714; loss_train 0.0877; loss_dev 0.0988\n","Ep. 3/5; It. 100/2608; Runtime 111.40 s; Remaining runtime 2h23min24s; f1_train 0.6897; f1_dev 0.5895; loss_train 0.0900; loss_dev 0.0964\n","Ep. 3/5; It. 200/2608; Runtime 110.14 s; Remaining runtime 2h19min56s; f1_train 0.6471; f1_dev 0.6777; loss_train 0.1299; loss_dev 0.0928\n","Ep. 3/5; It. 300/2608; Runtime 114.07 s; Remaining runtime 2h23min2s; f1_train 0.5806; f1_dev 0.5895; loss_train 0.0677; loss_dev 0.0936\n","Ep. 3/5; It. 400/2608; Runtime 111.57 s; Remaining runtime 2h18min2s; f1_train 0.8333; f1_dev 0.6545; loss_train 0.0621; loss_dev 0.0950\n","Ep. 3/5; It. 500/2608; Runtime 109.56 s; Remaining runtime 2h13min44s; f1_train 0.7188; f1_dev 0.6333; loss_train 0.0939; loss_dev 0.0963\n","Ep. 3/5; It. 600/2608; Runtime 112.03 s; Remaining runtime 2h14min53s; f1_train 0.6038; f1_dev 0.6226; loss_train 0.0924; loss_dev 0.0995\n","Ep. 3/5; It. 700/2608; Runtime 111.27 s; Remaining runtime 2h12min6s; f1_train 0.6122; f1_dev 0.6346; loss_train 0.0931; loss_dev 0.0966\n","Ep. 3/5; It. 800/2608; Runtime 110.21 s; Remaining runtime 2h9min1s; f1_train 0.7500; f1_dev 0.6415; loss_train 0.0800; loss_dev 0.1017\n","Ep. 3/5; It. 900/2608; Runtime 111.10 s; Remaining runtime 2h8min12s; f1_train 0.7273; f1_dev 0.6000; loss_train 0.0497; loss_dev 0.0955\n","Ep. 3/5; It. 1000/2608; Runtime 116.46 s; Remaining runtime 2h12min27s; f1_train 0.6122; f1_dev 0.6275; loss_train 0.0848; loss_dev 0.0945\n","Ep. 3/5; It. 1100/2608; Runtime 111.07 s; Remaining runtime 2h4min28s; f1_train 0.7606; f1_dev 0.6364; loss_train 0.0791; loss_dev 0.0956\n","Ep. 3/5; It. 1200/2608; Runtime 112.44 s; Remaining runtime 2h4min7s; f1_train 0.6977; f1_dev 0.5567; loss_train 0.0664; loss_dev 0.0996\n","Ep. 3/5; It. 1300/2608; Runtime 109.90 s; Remaining runtime 1h59min29s; f1_train 0.7077; f1_dev 0.6250; loss_train 0.0860; loss_dev 0.0951\n","Ep. 3/5; It. 1400/2608; Runtime 111.04 s; Remaining runtime 1h58min53s; f1_train 0.6000; f1_dev 0.6168; loss_train 0.1035; loss_dev 0.0916\n","Ep. 3/5; It. 1500/2608; Runtime 110.57 s; Remaining runtime 1h56min32s; f1_train 0.6786; f1_dev 0.6667; loss_train 0.0913; loss_dev 0.0908\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-cdd880e74745>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mcycle_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_lr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_lr\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin_lr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mX_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQIQC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcycle_lr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m30\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"5fAfqgZZyAtW","colab_type":"code","colab":{}},"cell_type":"code","source":["MODELS_FILE = \"models/model3.3/QIQC_model3.3.ckpt\"\n","modfil = os.path.join(DRIVE_PATH,MODELS_FILE)\n","X_cv_lengths, X_cv_set, y_cv_set = QIQC.cross_val_set()\n","\n","with tf.Session() as sess: \n","    saver.restore(sess, modfil)\n","    f1_cv = f1.eval(feed_dict = {X : X_cv_set, y : y_cv_set, seq_len: X_cv_lengths})\n","    f1_dev = f1.eval(feed_dict = {X : X_dev_set, y : y_dev_set, seq_len: X_dev_lengths})\n","\n","f1_cv_pc = f1_cv * 100\n","f1_dev_pc = f1_dev * 100\n","print(\"cross val f1 {:.2f}%, dev f1 {:.2f}%\".format(f1_cv_pc, f1_dev_pc))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9mk0AYC9zxlu","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}