{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"QIQC_model_4.3.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1546207887954,"user_tz":-120,"elapsed":31130,"user":{"displayName":"Quentin Legrand","photoUrl":"https://lh6.googleusercontent.com/-IXHiOGs04sE/AAAAAAAAAAI/AAAAAAAAAFk/ppsEvqzCcGw/s64/photo.jpg","userId":"00421493000358847389"}},"id":"ODRJ7bX8vV9M","outputId":"bcf36795-fa25-460d-d3f5-0e8dcd4a1744","colab":{"base_uri":"https://localhost:8080/","height":129}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","id":"CYb4xYXtV75L","outputId":"7ff5a2b5-6985-4431-b6bb-9126f5c68df9","executionInfo":{"status":"ok","timestamp":1546207889145,"user_tz":-120,"elapsed":32162,"user":{"displayName":"Quentin Legrand","photoUrl":"https://lh6.googleusercontent.com/-IXHiOGs04sE/AAAAAAAAAAI/AAAAAAAAAFk/ppsEvqzCcGw/s64/photo.jpg","userId":"00421493000358847389"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["import pandas as pd\n","import gc\n","import numpy as np\n","\n","import tensorflow as tf\n","import keras\n","from keras import layers, activations, models, optimizers, utils, regularizers\n","import keras.backend as backend\n","from keras.engine.input_layer import Input\n","from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau\n","from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n","\n","\n","import os\n","from datetime import datetime as dt\n","import time\n","import re\n","from functools import partial\n","\n","%matplotlib inline\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","version = \"4.3\"\n","\n","DRIVE_PATH = \"/content/gdrive/My Drive/Colab Notebooks/\"\n","TRAIN_FILE = \"datasets/train.csv\"\n","CORRECTED_TRAIN_FILE = \"datasets/corrected_train.csv\"\n","TEST_FILE = \"datasets/test.csv\"\n","GLOVE_FILE = \"embeddings/glove.840B.300d/glove.840B.300d.txt\"\n","MODEL_SAVE_PATH = \"models/model{}/QIQC_model{}.h5\".format(version,version)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1546207890540,"user_tz":-120,"elapsed":33457,"user":{"displayName":"Quentin Legrand","photoUrl":"https://lh6.googleusercontent.com/-IXHiOGs04sE/AAAAAAAAAAI/AAAAAAAAAFk/ppsEvqzCcGw/s64/photo.jpg","userId":"00421493000358847389"}},"id":"_HRVliA5WoLj","outputId":"bd70b86c-8556-4e2c-e3f8-020cb313c9db","colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"8PHq3bJ-V75f"},"cell_type":"markdown","source":["# Toolbox"]},{"metadata":{"colab_type":"code","id":"xIBMtV8dV75k","colab":{}},"cell_type":"code","source":["def reset_graph(seed=42):\n","    tf.reset_default_graph()\n","    tf.set_random_seed(seed)\n","    np.random.seed(seed)\n","\n","def load_data(path = DRIVE_PATH, file = CORRECTED_TRAIN_FILE):\n","    csv_path = os.path.join(path,file)\n","    return pd.read_csv(csv_path)\n","\n","def tokenize(sentences_list):\n","    return [re.findall(r\"[\\w]+|[']|[.,!?;]\", str(x)) for x in sentences_list] \n","    \n","def get_vocab(sentences):\n","    vocab={}\n","    for sentence in sentences:\n","        for word in sentence:\n","            try:\n","                vocab[word] +=1\n","            except KeyError:\n","                vocab[word] = 1\n","    return vocab\n","\n","def glove_embeddings(vocabulary_in_set, drive = DRIVE_PATH, gloveFile = GLOVE_FILE ,extract = -1):\n","\n","    glove_file = os.path.join(drive,gloveFile)\n","  \n","    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n","\n","    embeddings = []\n","    words_id = {}\n","    f = open(glove_file,'r', encoding=\"utf8\")\n","    increment = 0\n","    words_id[\"\"]=0\n","    first = True\n","    i = 1\n","    \n","    for line in f:\n","        word, vect = get_coefs(*line.split(\" \"))\n","        if first:\n","            embeddings.append(np.zeros_like(vect))\n","            first = False\n","        if word in vocabulary_in_set:\n","            embeddings.append(vect)\n","            words_id[word] = i\n","            i += 1\n","            if increment == extract - 1:\n","                break\n","            elif extract != -1:\n","                increment += 1\n","    f.close()   \n","    return np.array(embeddings), words_id\n","\n","def df_to_data_target(df, data_col = \"corrected_question_text\", target_col = \"target\"):\n","    data = df[data_col]\n","    target = df[target_col]\n","    return np.c_[data, target]\n","\n","def embed_and_pad(X, embeddings, n_dims, width):\n","    padded_X = np.zeros((len(X),width,n_dims))\n","    i = 0\n","    for sentence in X:\n","        j = 0\n","        for word in sentence:\n","            padded_X[i,j,:] = embeddings[word]\n","            j += 1\n","        i +=1\n","    return padded_X\n","\n","def numpy_ewma_vectorized(data, alpha):\n","    alpha_ = 1-alpha\n","    D = pd.DataFrame(data)\n","    M = D.ewm(alpha=alpha_).mean()\n","    out = M.values\n","    return out\n","  \n","def split_data(data_targets, dv, cv):\n","        positive_data_targets = data_targets[data_targets[:,1]==1]\n","        negative_data_targets = data_targets[data_targets[:,1]==0]\n","        \n","        positive_length = len(positive_data_targets)\n","        negative_length = len(negative_data_targets)\n","  \n","        pos_data_targets = positive_data_targets[np.random.permutation(positive_length), :]\n","        neg_data_targets = negative_data_targets[np.random.permutation(negative_length), :]\n","        \n","        if dv != 0:\n","            dv_positive = int(dv * positive_length/(positive_length + negative_length))\n","            dv_negative = dv - dv_positive\n","            dv_data_targets = np.concatenate((positive_data_targets[0:dv_positive,:], negative_data_targets[0:dv_negative,:]))\n","            dv_data_targets = dv_data_targets[np.random.permutation(dv), :]\n","            y_dev = dv_data_targets[:,1].astype(float)\n","            X_dev = dv_data_targets[:,0]\n","        else:\n","            dv_positive = 0\n","            dv_negative = 0\n","            \n","        if cv != 0:\n","            cv_positive = int(cv * positive_length/(positive_length + negative_length))\n","            cv_negative = cv - cv_positive\n","            cv_data_targets = np.concatenate((positive_data_targets[dv_positive:dv_positive + cv_positive,:], negative_data_targets[dv_negative:dv_negative + cv_negative,:]))\n","            cv_data_targets = cv_data_targets[np.random.permutation(cv), :]\n","            y_cross = cv_data_targets[:,1].astype(float)\n","            X_cross = cv_data_targets[:,0]\n","        else:\n","            cv_positive = 0\n","            cv_negative = 0\n","            \n","        train_data_targets = np.concatenate((positive_data_targets[dv_positive + cv_positive:,:], negative_data_targets[dv_negative + cv_negative:,:]))\n","        train_data_targets = train_data_targets[np.random.permutation(len(data_targets)-dv-cv), :]\n","        y_train = train_data_targets[:,1].astype(float)\n","        X_train = train_data_targets[:,0]\n","        \n","        if cv == 0 and dv == 0:\n","          return X_train, y_train\n","        elif cv == 0 and dv != 0:\n","          return X_train, y_train, X_dev, y_dev\n","        elif cv != 0 and dv == 0:\n","          return X_train, y_train, X_cross, y_cross\n","        else:\n","          return X_train, y_train, X_dev, y_dev, X_cross, y_cross\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aPpDqM_1RVLK","colab_type":"code","colab":{}},"cell_type":"code","source":["class QIQCSequence(utils.Sequence):\n","\n","    def __init__(self, x_set, y_set, batch_size, words_ids, seed=42):\n","        Xs = self.__tokenize(x_set)\n","        self.x = self.__ids_and_pad(Xs ,words_ids[0], words_ids[1])\n","        self.y = y_set\n","        self.batch_size = batch_size\n","        self.seed = seed\n","        np.random.seed(seed)\n","        self.pointer = 0\n","        self.reshuffle_seed = 0\n","\n","    def __len__(self):\n","        return int(np.ceil(len(self.x) / float(self.batch_size)))\n","    \n","    def __tokenize(self, X):\n","        return [re.findall(r\"[\\w]+|[']|[.,!?;]\", str(x)) for x in X]\n","      \n","    def __ids_and_pad(self, X, word_ids, width):\n","        padded_X = np.zeros((len(X),width))\n","        i = 0\n","        for sentence in X:\n","            j = 0\n","            for word in sentence:\n","                padded_X[i,j] = word_ids[word]\n","                j += 1\n","            i +=1\n","        return padded_X\n","    \n","    def on_epoch_end(self):\n","        self.pointer = 0\n","        self.reshuffle_seed += 1\n","        self.indexes = np.arange(len(self.y))\n","        np.random.shuffle(self.indexes)\n","        np.random.seed(self.seed + self.reshuffle_seed)\n","\n","    def __getitem__(self, idx): \n","        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n","        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]        \n","            \n","        return np.array(batch_x), np.array(batch_y)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1mM_KbL8MNle","colab_type":"code","colab":{}},"cell_type":"code","source":["class Metrics(Callback):\n","\n","  def on_train_begin(self, logs={}):\n","    self.val_f1s = []\n","    self.val_recalls = []\n","    self.val_precisions = []\n","    \n","  def on_epoch_end(self, epoch, logs={}):\n","    val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n","    val_targ = self.validation_data[1]\n","    _val_f1 = f1_score(val_targ, val_predict)\n","    _val_recall = recall_score(val_targ, val_predict)\n","    _val_precision = precision_score(val_targ, val_predict)\n","    self.val_f1s.append(_val_f1)\n","    self.val_recalls.append(_val_recall)\n","    self.val_precisions.append(_val_precision)\n","    print(\" — val_f1: {} — val_precision: {} — val_recall {}\".format(_val_f1, _val_precision, _val_recall))\n","    return"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"8OJMLwdPV757"},"cell_type":"markdown","source":["# Load and prepare data"]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1546207895735,"user_tz":-120,"elapsed":38263,"user":{"displayName":"Quentin Legrand","photoUrl":"https://lh6.googleusercontent.com/-IXHiOGs04sE/AAAAAAAAAAI/AAAAAAAAAFk/ppsEvqzCcGw/s64/photo.jpg","userId":"00421493000358847389"}},"id":"06xzg79TV75-","outputId":"aae5b045-44d6-4265-ea45-c2a8f9ffc93d","colab":{"base_uri":"https://localhost:8080/","height":206}},"cell_type":"code","source":["train_data = load_data()\n","train_data.head()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>qid</th>\n","      <th>target</th>\n","      <th>corrected_question_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>00002165364db923c7e6</td>\n","      <td>0</td>\n","      <td>How did Quebec nationalists see their province...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000032939017120e6e44</td>\n","      <td>0</td>\n","      <td>Do you have an adopted dog , how would you enc...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0000412ca6e4628ce2cf</td>\n","      <td>0</td>\n","      <td>Why does velocity affect time ? Does velocity ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>000042bf85aa498cd78e</td>\n","      <td>0</td>\n","      <td>How did Otto von Guericke used the Magdeburg h...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0000455dfa3e01eae3af</td>\n","      <td>0</td>\n","      <td>Can I convert montra helicon D to a mountain b...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                    qid  target  \\\n","0  00002165364db923c7e6       0   \n","1  000032939017120e6e44       0   \n","2  0000412ca6e4628ce2cf       0   \n","3  000042bf85aa498cd78e       0   \n","4  0000455dfa3e01eae3af       0   \n","\n","                             corrected_question_text  \n","0  How did Quebec nationalists see their province...  \n","1  Do you have an adopted dog , how would you enc...  \n","2  Why does velocity affect time ? Does velocity ...  \n","3  How did Otto von Guericke used the Magdeburg h...  \n","4  Can I convert montra helicon D to a mountain b...  "]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"colab_type":"code","id":"TFTmNHrfV76L","colab":{}},"cell_type":"code","source":["data_targets = df_to_data_target(train_data)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"s8-E1GHuV76V","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"3ebb524a-74d3-4f6e-bf81-a7426a3ca0cd","executionInfo":{"status":"ok","timestamp":1546207911022,"user_tz":-120,"elapsed":53373,"user":{"displayName":"Quentin Legrand","photoUrl":"https://lh6.googleusercontent.com/-IXHiOGs04sE/AAAAAAAAAAI/AAAAAAAAAFk/ppsEvqzCcGw/s64/photo.jpg","userId":"00421493000358847389"}}},"cell_type":"code","source":["tokenized_questions = tokenize(train_data[\"corrected_question_text\"].values)\n","vocabulary_in_set = get_vocab(tokenized_questions)\n","del train_data, tokenized_questions\n","gc.collect()"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["33"]},"metadata":{"tags":[]},"execution_count":9}]},{"metadata":{"colab_type":"code","id":"31VolUA5V76l","colab":{}},"cell_type":"code","source":["embeddings, words_ids = glove_embeddings(vocabulary_in_set=vocabulary_in_set)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"pOVK2aYgV77U"},"cell_type":"markdown","source":["# Construction phase"]},{"metadata":{"colab_type":"code","id":"er4v1HWSV77a","outputId":"3272ba2f-ff30-4bf8-bf3d-80357ef9d4e5","executionInfo":{"status":"ok","timestamp":1546208138743,"user_tz":-120,"elapsed":280907,"user":{"displayName":"Quentin Legrand","photoUrl":"https://lh6.googleusercontent.com/-IXHiOGs04sE/AAAAAAAAAAI/AAAAAAAAAFk/ppsEvqzCcGw/s64/photo.jpg","userId":"00421493000358847389"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"cell_type":"code","source":["\"\"\"reset_graph()\n","\n","n_steps = 250\n","window_width = [3, 4, 5]\n","out_channels = [n_steps-window_width[0]+1, n_steps-window_width[1]+1, n_steps-window_width[2]+1]\n","embedding_dimensions = 300\n","n_fc1 = 300\n","n_fc2 = 300\n","n_fc3 = 300\n","n_fc4 = 200\n","n_fc5 = 100\n","n_output = 1\n","threshold = 1.\n","lambda_ = 0.05\n","\n","activation_fn = tf.nn.elu\n","he_init = tf.variance_scaling_initializer()\n","\n","init_W_CL_3 = np.random.randn(window_width[0],embedding_dimensions,1,out_channels[0]).astype(np.float32)\n","init_b_CL_3 = np.random.randn(1,1,1,out_channels[0]).astype(np.float32)\n","init_W_CL_4 = np.random.randn(window_width[1],embedding_dimensions,1,out_channels[1]).astype(np.float32)\n","init_b_CL_4 = np.random.randn(1,1,1,out_channels[1]).astype(np.float32)\n","init_W_CL_5 = np.random.randn(window_width[2],embedding_dimensions,1,out_channels[2]).astype(np.float32)\n","init_b_CL_5 = np.random.randn(1,1,1,out_channels[2]).astype(np.float32)\n","\n","\n","X = tf.placeholder(tf.int32, [None, n_steps], name=\"X\") #m * n_steps\n","y = tf.placeholder(tf.float32, [None], name=\"y\")\n","learning_rate = tf.placeholder(tf.float32)\n","#training = tf.placeholder_with_default(False, shape=(), name='training')\n","\n","with tf.name_scope(\"embedding_layer\"):\n","    word_embeddings = tf.Variable(initial_value=embeddings, trainable=False)# n_words * embedding_dimensions\n","    embedded_sentences = tf.nn.embedding_lookup(word_embeddings, X) # m * n_steps * embedding_dimensions\n","\n","with tf.name_scope(\"CNN_3\"):\n","    W_CL_3 = tf.Variable(initial_value=init_W_CL_3)\n","    b_CL_3 = tf.Variable(initial_value=init_b_CL_3)\n","    CL_3 = tf.nn.conv2d(tf.reshape(embedded_sentences,[-1,n_steps,embedding_dimensions,1]), W_CL_3, strides=[1,1,1,1], padding=\"VALID\")# m * n_steps - 3 + 1 *1 * out_channels \n","    activated_CL_3 = activation_fn(tf.add(CL_3,b_CL_3)) # m * n_steps - 5 + 1 * 1 * out_channels\n","    MAXPOOL_3 = tf.reshape(tf.math.reduce_max(activated_CL_3,axis=1),[-1,out_channels[0]]) # m * 1 * out_channels  --> m * out_channels\n","    \n","with tf.name_scope(\"CNN_4\"):\n","    W_CL_4 = tf.Variable(initial_value=init_W_CL_4)\n","    b_CL_4 = tf.Variable(initial_value=init_b_CL_4)\n","    CL_4 = tf.nn.conv2d(tf.reshape(embedded_sentences,[-1,n_steps,embedding_dimensions,1]), W_CL_4, strides=[1,1,1,1], padding=\"VALID\")# m * n_steps - 4 + 1 *1 * out_channels \n","    activated_CL_4 = activation_fn(tf.add(CL_4,b_CL_4)) # m * n_steps - 5 + 1 * 1 * out_channels\n","    MAXPOOL_4 = tf.reshape(tf.math.reduce_max(activated_CL_4,axis=1),[-1,out_channels[1]]) # m * 1 * out_channels  --> m * out_channels\n","    \n","with tf.name_scope(\"CNN_5\"):\n","    W_CL_5 = tf.Variable(initial_value=init_W_CL_5)\n","    b_CL_5 = tf.Variable(initial_value=init_b_CL_5)\n","    CL_5 = tf.nn.conv2d(tf.reshape(embedded_sentences,[-1,n_steps,embedding_dimensions,1]), W_CL_5, strides=[1,1,1,1], padding=\"VALID\")# m * n_steps - 5 + 1 *1 * out_channels \n","    activated_CL_5 = activation_fn(tf.add(CL_5,b_CL_5)) # m * n_steps - 5 + 1 * 1 * out_channels\n","    MAXPOOL_5 = tf.reshape(tf.math.reduce_max(activated_CL_5,axis=1),[-1,out_channels[2]]) # m * 1 * out_channels  --> m * out_channels\n","\n","with tf.name_scope(\"Max_pool\"):\n","    MAXPOOL = tf.concat([MAXPOOL_3,MAXPOOL_4,MAXPOOL_5], axis=1)\n","    \n","with tf.name_scope(\"fully_connected_layers\"):\n","    fc_layer = partial(tf.layers.dense, activation=activation_fn, kernel_regularizer=tf.contrib.layers.l2_regularizer(lambda_), kernel_initializer=he_init)\n","    fc1 = fc_layer(MAXPOOL, n_fc1)\n","    fc2 = fc_layer(fc1, n_fc2)\n","    fc3 = fc_layer(fc2, n_fc3)\n","    fc4 = fc_layer(fc3, n_fc4)\n","    fc5 = fc_layer(fc4, n_fc5)\n","\n","\n","with tf.name_scope(\"logits_and_outputs\"):\n","    logits = tf.reshape(tf.contrib.layers.fully_connected(fc5, n_output, activation_fn=None),shape=[-1])\n","    outputs = tf.sigmoid(logits)\n","    predictions = tf.round(outputs)\n","\n","with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n","    base_loss = tf.reduce_mean(xentropy)\n","    reg_loss = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n","    loss = tf.add_n([base_loss] + reg_loss)\n","\n","with tf.name_scope(\"evaluation\"):\n","    TP = tf.count_nonzero(predictions * y)\n","    TN = tf.count_nonzero((predictions - 1) * (y - 1))\n","    FP = tf.count_nonzero(predictions * (y - 1))\n","    FN = tf.count_nonzero((predictions - 1) * y)\n","    precision = TP / (TP + FP)\n","    recall = TP / (TP + FN)\n","    accuracy = (TP + TN)/(TP + FP + TN + FN)\n","    f1 = 2 * precision * recall / (precision + recall)\n","\n","with tf.name_scope(\"training_op\"):\n","    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","    grads_and_vars = optimizer.compute_gradients(loss)\n","    capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n","    l2_grads = tf.norm(grads_and_vars[0])\n","    l2_capped_grads = tf.norm(capped_gvs[0])\n","    training_op = optimizer.apply_gradients(capped_gvs)\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()\n","\n","f1_train_summary = tf.summary.scalar('Train_F1_score', f1)\n","f1_dev_summary = tf.summary.scalar(\"Dev_F1_score\", f1)\n","loss_train_summary = tf.summary.scalar(\"Train_loss\", loss)\n","loss_dev_summary = tf.summary.scalar(\"Dev_loss\", loss)\"\"\""],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'reset_graph()\\n\\nn_steps = 250\\nwindow_width = [3, 4, 5]\\nout_channels = [n_steps-window_width[0]+1, n_steps-window_width[1]+1, n_steps-window_width[2]+1]\\nembedding_dimensions = 300\\nn_fc1 = 300\\nn_fc2 = 300\\nn_fc3 = 300\\nn_fc4 = 200\\nn_fc5 = 100\\nn_output = 1\\nthreshold = 1.\\nlambda_ = 0.05\\n\\nactivation_fn = tf.nn.elu\\nhe_init = tf.variance_scaling_initializer()\\n\\ninit_W_CL_3 = np.random.randn(window_width[0],embedding_dimensions,1,out_channels[0]).astype(np.float32)\\ninit_b_CL_3 = np.random.randn(1,1,1,out_channels[0]).astype(np.float32)\\ninit_W_CL_4 = np.random.randn(window_width[1],embedding_dimensions,1,out_channels[1]).astype(np.float32)\\ninit_b_CL_4 = np.random.randn(1,1,1,out_channels[1]).astype(np.float32)\\ninit_W_CL_5 = np.random.randn(window_width[2],embedding_dimensions,1,out_channels[2]).astype(np.float32)\\ninit_b_CL_5 = np.random.randn(1,1,1,out_channels[2]).astype(np.float32)\\n\\n\\nX = tf.placeholder(tf.int32, [None, n_steps], name=\"X\") #m * n_steps\\ny = tf.placeholder(tf.float32, [None], name=\"y\")\\nlearning_rate = tf.placeholder(tf.float32)\\n#training = tf.placeholder_with_default(False, shape=(), name=\\'training\\')\\n\\nwith tf.name_scope(\"embedding_layer\"):\\n    word_embeddings = tf.Variable(initial_value=embeddings, trainable=False)# n_words * embedding_dimensions\\n    embedded_sentences = tf.nn.embedding_lookup(word_embeddings, X) # m * n_steps * embedding_dimensions\\n\\nwith tf.name_scope(\"CNN_3\"):\\n    W_CL_3 = tf.Variable(initial_value=init_W_CL_3)\\n    b_CL_3 = tf.Variable(initial_value=init_b_CL_3)\\n    CL_3 = tf.nn.conv2d(tf.reshape(embedded_sentences,[-1,n_steps,embedding_dimensions,1]), W_CL_3, strides=[1,1,1,1], padding=\"VALID\")# m * n_steps - 3 + 1 *1 * out_channels \\n    activated_CL_3 = activation_fn(tf.add(CL_3,b_CL_3)) # m * n_steps - 5 + 1 * 1 * out_channels\\n    MAXPOOL_3 = tf.reshape(tf.math.reduce_max(activated_CL_3,axis=1),[-1,out_channels[0]]) # m * 1 * out_channels  --> m * out_channels\\n    \\nwith tf.name_scope(\"CNN_4\"):\\n    W_CL_4 = tf.Variable(initial_value=init_W_CL_4)\\n    b_CL_4 = tf.Variable(initial_value=init_b_CL_4)\\n    CL_4 = tf.nn.conv2d(tf.reshape(embedded_sentences,[-1,n_steps,embedding_dimensions,1]), W_CL_4, strides=[1,1,1,1], padding=\"VALID\")# m * n_steps - 4 + 1 *1 * out_channels \\n    activated_CL_4 = activation_fn(tf.add(CL_4,b_CL_4)) # m * n_steps - 5 + 1 * 1 * out_channels\\n    MAXPOOL_4 = tf.reshape(tf.math.reduce_max(activated_CL_4,axis=1),[-1,out_channels[1]]) # m * 1 * out_channels  --> m * out_channels\\n    \\nwith tf.name_scope(\"CNN_5\"):\\n    W_CL_5 = tf.Variable(initial_value=init_W_CL_5)\\n    b_CL_5 = tf.Variable(initial_value=init_b_CL_5)\\n    CL_5 = tf.nn.conv2d(tf.reshape(embedded_sentences,[-1,n_steps,embedding_dimensions,1]), W_CL_5, strides=[1,1,1,1], padding=\"VALID\")# m * n_steps - 5 + 1 *1 * out_channels \\n    activated_CL_5 = activation_fn(tf.add(CL_5,b_CL_5)) # m * n_steps - 5 + 1 * 1 * out_channels\\n    MAXPOOL_5 = tf.reshape(tf.math.reduce_max(activated_CL_5,axis=1),[-1,out_channels[2]]) # m * 1 * out_channels  --> m * out_channels\\n\\nwith tf.name_scope(\"Max_pool\"):\\n    MAXPOOL = tf.concat([MAXPOOL_3,MAXPOOL_4,MAXPOOL_5], axis=1)\\n    \\nwith tf.name_scope(\"fully_connected_layers\"):\\n    fc_layer = partial(tf.layers.dense, activation=activation_fn, kernel_regularizer=tf.contrib.layers.l2_regularizer(lambda_), kernel_initializer=he_init)\\n    fc1 = fc_layer(MAXPOOL, n_fc1)\\n    fc2 = fc_layer(fc1, n_fc2)\\n    fc3 = fc_layer(fc2, n_fc3)\\n    fc4 = fc_layer(fc3, n_fc4)\\n    fc5 = fc_layer(fc4, n_fc5)\\n\\n\\nwith tf.name_scope(\"logits_and_outputs\"):\\n    logits = tf.reshape(tf.contrib.layers.fully_connected(fc5, n_output, activation_fn=None),shape=[-1])\\n    outputs = tf.sigmoid(logits)\\n    predictions = tf.round(outputs)\\n\\nwith tf.name_scope(\"loss\"):\\n    xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\\n    base_loss = tf.reduce_mean(xentropy)\\n    reg_loss = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\\n    loss = tf.add_n([base_loss] + reg_loss)\\n\\nwith tf.name_scope(\"evaluation\"):\\n    TP = tf.count_nonzero(predictions * y)\\n    TN = tf.count_nonzero((predictions - 1) * (y - 1))\\n    FP = tf.count_nonzero(predictions * (y - 1))\\n    FN = tf.count_nonzero((predictions - 1) * y)\\n    precision = TP / (TP + FP)\\n    recall = TP / (TP + FN)\\n    accuracy = (TP + TN)/(TP + FP + TN + FN)\\n    f1 = 2 * precision * recall / (precision + recall)\\n\\nwith tf.name_scope(\"training_op\"):\\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\\n    grads_and_vars = optimizer.compute_gradients(loss)\\n    capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\\n    l2_grads = tf.norm(grads_and_vars[0])\\n    l2_capped_grads = tf.norm(capped_gvs[0])\\n    training_op = optimizer.apply_gradients(capped_gvs)\\n\\ninit = tf.global_variables_initializer()\\nsaver = tf.train.Saver()\\n\\nf1_train_summary = tf.summary.scalar(\\'Train_F1_score\\', f1)\\nf1_dev_summary = tf.summary.scalar(\"Dev_F1_score\", f1)\\nloss_train_summary = tf.summary.scalar(\"Train_loss\", loss)\\nloss_dev_summary = tf.summary.scalar(\"Dev_loss\", loss)'"]},"metadata":{"tags":[]},"execution_count":11}]},{"metadata":{"id":"eTRarso3CisS","colab_type":"code","outputId":"79f8fa32-8994-4e32-ef43-e8b8ef851609","executionInfo":{"status":"ok","timestamp":1546208139758,"user_tz":-120,"elapsed":281763,"user":{"displayName":"Quentin Legrand","photoUrl":"https://lh6.googleusercontent.com/-IXHiOGs04sE/AAAAAAAAAAI/AAAAAAAAAFk/ppsEvqzCcGw/s64/photo.jpg","userId":"00421493000358847389"}},"colab":{"base_uri":"https://localhost:8080/","height":880}},"cell_type":"code","source":["n_fc = [300, 300, 300, 200, 100, 1]\n","n_steps = 250\n","n_dims = 300\n","window_width = [3, 4, 5]\n","out_channels = [100,100,100]#[n_steps-window_width[0]+1, n_steps-window_width[1]+1, n_steps-window_width[2]+1]\n","activation = activations.elu\n","\n","lambda_ = 0.01\n","lr = 0.004\n","\n","X = Input(shape=(n_steps,))\n","\n","embedding = layers.Embedding(len(words_ids), n_dims, input_length=n_steps, weights=[embeddings], trainable=False)(X)# m x n_steps x 300\n","embedding = layers.Reshape((n_steps,n_dims,1))(embedding)\n","\n","conv_3 = layers.Conv2D(out_channels[0], (window_width[0], n_dims), activation=activation)(embedding) #m x (n_steps-window_width[0]+1) x 1 x out_channel[0]\n","conv_3 = layers.MaxPooling2D(pool_size=((n_steps-window_width[0]+1) ,1))(conv_3) #m x 1 x 1 x out_channels[0]\n","conv_3 = layers.Reshape((out_channels[0],))(conv_3)\n","\n","conv_4 = layers.Conv2D(out_channels[1], (window_width[1], n_dims), activation=activation)(embedding)\n","conv_4 = layers.MaxPooling2D(pool_size=((n_steps-window_width[1]+1) ,1))(conv_4)\n","conv_4 = layers.Reshape((out_channels[1],))(conv_4)\n","\n","conv_5 = layers.Conv2D(out_channels[2], (window_width[2], n_dims), activation=activation)(embedding)\n","conv_5 = layers.MaxPooling2D(pool_size=((n_steps-window_width[2]+1),1))(conv_5)\n","conv_5 = layers.Reshape((out_channels[2],))(conv_5)\n","\n","merged = layers.concatenate([conv_3, conv_4, conv_5], axis=1)\n","\n","fc = layers.Dense(n_fc[0], activation=activation, kernel_regularizer=regularizers.l2(lambda_))(merged)\n","fc = layers.Dense(n_fc[1], activation=activation, kernel_regularizer=regularizers.l2(lambda_))(fc)\n","fc = layers.Dense(n_fc[2], activation=activation, kernel_regularizer=regularizers.l2(lambda_))(fc)\n","fc = layers.Dense(n_fc[3], activation=activation, kernel_regularizer=regularizers.l2(lambda_))(fc)\n","fc = layers.Dense(n_fc[4], activation=activation, kernel_regularizer=regularizers.l2(lambda_))(fc)\n","\n","out = layers.Dense(n_fc[5], activation='sigmoid')(fc)\n","\n","model = models.Model(inputs=X, outputs=out)\n","optimizer = optimizers.Adam(lr=lr)\n","\n","metrics = Metrics()\n","checkpoint = ModelCheckpoint(os.path.join(DRIVE_PATH, MODEL_SAVE_PATH), monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, min_lr=0.00001, verbose=2)\n","\n","model.compile(loss='binary_crossentropy',optimizer=optimizer)\n","model.summary()"],"execution_count":12,"outputs":[{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 250)          0                                            \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, 250, 300)     53934300    input_1[0][0]                    \n","__________________________________________________________________________________________________\n","reshape_1 (Reshape)             (None, 250, 300, 1)  0           embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 248, 1, 100)  90100       reshape_1[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 247, 1, 100)  120100      reshape_1[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 246, 1, 100)  150100      reshape_1[0][0]                  \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","reshape_2 (Reshape)             (None, 100)          0           max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","reshape_3 (Reshape)             (None, 100)          0           max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","reshape_4 (Reshape)             (None, 100)          0           max_pooling2d_3[0][0]            \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 300)          0           reshape_2[0][0]                  \n","                                                                 reshape_3[0][0]                  \n","                                                                 reshape_4[0][0]                  \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 300)          90300       concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 300)          90300       dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 300)          90300       dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 200)          60200       dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","dense_5 (Dense)                 (None, 100)          20100       dense_4[0][0]                    \n","__________________________________________________________________________________________________\n","dense_6 (Dense)                 (None, 1)            101         dense_5[0][0]                    \n","==================================================================================================\n","Total params: 54,645,901\n","Trainable params: 711,601\n","Non-trainable params: 53,934,300\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"-66y9w6WV77g"},"cell_type":"markdown","source":["# Training phase"]},{"metadata":{"colab_type":"code","id":"MayLFImnV77p","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"ad182d04-5651-440e-e00f-48c54339d145","executionInfo":{"status":"ok","timestamp":1546208159273,"user_tz":-120,"elapsed":301155,"user":{"displayName":"Quentin Legrand","photoUrl":"https://lh6.googleusercontent.com/-IXHiOGs04sE/AAAAAAAAAAI/AAAAAAAAAFk/ppsEvqzCcGw/s64/photo.jpg","userId":"00421493000358847389"}}},"cell_type":"code","source":["batch_size = 64\n","\n","X_train, y_train, X_dev, y_dev = split_data(data_targets, dv=2000, cv=0)\n","QIQC_train = QIQCSequence(X_train, y_train, batch_size=batch_size, words_ids=(words_ids,n_steps))\n","QIQC_dev = QIQCSequence(X_dev, y_dev, batch_size=2000, words_ids=(words_ids,n_steps))\n","X_dev, y_dev = QIQC_dev.__getitem__(0)\n","del embeddings, words_ids\n","gc.collect()"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":13}]},{"metadata":{"id":"Eatk3tWICisy","colab_type":"code","outputId":"af972f41-8192-44d5-fda2-3b7789b77479","executionInfo":{"status":"error","timestamp":1546014901920,"user_tz":-120,"elapsed":1776123,"user":{"displayName":"Quentin Legrand","photoUrl":"https://lh6.googleusercontent.com/-IXHiOGs04sE/AAAAAAAAAAI/AAAAAAAAAFk/ppsEvqzCcGw/s64/photo.jpg","userId":"00421493000358847389"}},"colab":{"base_uri":"https://localhost:8080/","height":495}},"cell_type":"code","source":["n_epochs = 8\n","model.fit_generator(QIQC_train, epochs=n_epochs, verbose=1, validation_data=(X_dev, y_dev), callbacks=[metrics, checkpoint, reduce_lr])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/8\n","20377/20377 [==============================] - 984s 48ms/step - loss: 0.7313 - val_loss: 0.1391\n"," — val_f1: 0.5555555555555556 — val_precision: 0.6451612903225806 — val_recall 0.4878048780487805\n","\n","Epoch 00001: val_loss improved from inf to 0.13913, saving model to /content/gdrive/My Drive/Colab Notebooks/models/model4.3/QIQC_model4.3.h5\n","Epoch 2/8\n","20377/20377 [==============================] - 981s 48ms/step - loss: 0.6407 - val_loss: 0.3270\n"," — val_f1: 0.34394904458598724 — val_precision: 0.7941176470588235 — val_recall 0.21951219512195122\n","\n","Epoch 00002: val_loss did not improve from 0.13913\n","\n","Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0020000000949949026.\n","Epoch 3/8\n","20377/20377 [==============================] - 981s 48ms/step - loss: 0.1427 - val_loss: 0.1220\n"," — val_f1: 0.6278026905829596 — val_precision: 0.7 — val_recall 0.5691056910569106\n","\n","Epoch 00003: val_loss improved from 0.13913 to 0.12201, saving model to /content/gdrive/My Drive/Colab Notebooks/models/model4.3/QIQC_model4.3.h5\n","Epoch 4/8\n","20377/20377 [==============================] - 990s 49ms/step - loss: 0.1897 - val_loss: 0.1224\n"," — val_f1: 0.5714285714285714 — val_precision: 0.7671232876712328 — val_recall 0.45528455284552843\n","\n","Epoch 00004: val_loss did not improve from 0.12201\n","\n","Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n","Epoch 5/8\n","  524/20377 [..............................] - ETA: 16:17 - loss: 0.1096"],"name":"stdout"}]},{"metadata":{"id":"PUz7jPmDM-y3","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}