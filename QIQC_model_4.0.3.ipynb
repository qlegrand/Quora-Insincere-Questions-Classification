{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"QIQC_model_4.0.3.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1545327165024,"user_tz":-120,"elapsed":38474,"user":{"displayName":"Quentin Legrand","photoUrl":"https://lh6.googleusercontent.com/-IXHiOGs04sE/AAAAAAAAAAI/AAAAAAAAAFk/ppsEvqzCcGw/s64/photo.jpg","userId":"00421493000358847389"}},"id":"ODRJ7bX8vV9M","outputId":"a1dd692c-4010-4d02-faf7-d7b18157ff53","colab":{"base_uri":"https://localhost:8080/","height":129}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","id":"CYb4xYXtV75L","colab":{}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import os\n","from datetime import datetime as dt\n","import time\n","import re\n","%matplotlib inline\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","DRIVE_PATH = \"/content/gdrive/My Drive/Colab Notebooks/\"\n","TRAIN_FILE = \"datasets/train.csv\"\n","CORRECTED_TRAIN_FILE = \"datasets/corrected_train.csv\"\n","TEST_FILE = \"datasets/test.csv\"\n","GLOVE_FILE = \"embeddings/glove.840B.300d/glove.840B.300d.txt\""],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1545327166881,"user_tz":-120,"elapsed":40275,"user":{"displayName":"Quentin Legrand","photoUrl":"https://lh6.googleusercontent.com/-IXHiOGs04sE/AAAAAAAAAAI/AAAAAAAAAFk/ppsEvqzCcGw/s64/photo.jpg","userId":"00421493000358847389"}},"id":"_HRVliA5WoLj","outputId":"a1d7443a-c221-48f3-eb4b-505d5e5d6756","colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"8PHq3bJ-V75f"},"cell_type":"markdown","source":["# Toolbox"]},{"metadata":{"colab_type":"code","id":"xIBMtV8dV75k","colab":{}},"cell_type":"code","source":["def reset_graph(seed=42):\n","    tf.reset_default_graph()\n","    tf.set_random_seed(seed)\n","    np.random.seed(seed)\n","\n","def load_data(path = DRIVE_PATH, file = CORRECTED_TRAIN_FILE):\n","    csv_path = os.path.join(path,file)\n","    return pd.read_csv(csv_path)\n","\n","def tokenize(sentences_list):\n","    return [re.findall(r\"[\\w]+|[']|[.,!?;]\", str(x)) for x in sentences_list] \n","    \n","def get_vocab(sentences):\n","    vocab={}\n","    for sentence in sentences:\n","        for word in sentence:\n","            try:\n","                vocab[word] +=1\n","            except KeyError:\n","                vocab[word] = 1\n","    return vocab\n","\n","def glove_embeddings(vocabulary_in_set, drive = DRIVE_PATH, gloveFile = GLOVE_FILE ,extract = -1):\n","\n","    glove_file = os.path.join(drive,gloveFile)\n","  \n","    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n","\n","    embeddings = []\n","    words_id = {}\n","    f = open(glove_file,'r', encoding=\"utf8\")\n","    increment = 0\n","    words_id[\"\"]=0\n","    first = True\n","    i = 1\n","    \n","    for line in f:\n","        word, vect = get_coefs(*line.split(\" \"))\n","        if first:\n","            embeddings.append(np.zeros_like(vect))\n","            first = False\n","        if word in vocabulary_in_set:\n","            embeddings.append(vect)\n","            words_id[word] = i\n","            i += 1\n","            if increment == extract - 1:\n","                break\n","            elif extract != -1:\n","                increment += 1\n","    f.close()   \n","    return np.array(embeddings), words_id\n","\n","def df_to_data_target(df, data_col = \"corrected_question_text\", target_col = \"target\"):\n","    data = df[data_col]\n","    target = df[target_col]\n","    return np.c_[data, target]\n","\n","def embed_and_pad(X, embeddings, n_dims, width):\n","    padded_X = np.zeros((len(X),width,n_dims))\n","    i = 0\n","    for sentence in X:\n","        j = 0\n","        for word in sentence:\n","            padded_X[i,j,:] = embeddings[word]\n","            j += 1\n","        i +=1\n","    return padded_X\n","\n","def numpy_ewma_vectorized(data, alpha):\n","    alpha_ = 1-alpha\n","    D = pd.DataFrame(data)\n","    M = D.ewm(alpha=alpha_).mean()\n","    out = M.values\n","    return out"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"xPTqJyieV75u","colab":{}},"cell_type":"code","source":["class data_split_and_batch:\n","    def __init__(self, data_targets, dv=0, cv=0, batch_size=500, words_ids = None, seed=42):\n","        self.seed = seed\n","        np.random.seed(seed)\n","        self.batch_length = batch_size\n","        self.pointer = 0\n","        self.reshuffle_seed = 0\n","        self.__split__(data_targets=data_targets, dv=dv, cv=cv, words_ids=words_ids)\n","    \n","    def __split__(self, data_targets, dv, cv, words_ids):\n","        positive_data_targets = data_targets[data_targets[:,1]==1]\n","        negative_data_targets = data_targets[data_targets[:,1]==0]\n","        \n","        positive_length = len(positive_data_targets)\n","        negative_length = len(negative_data_targets)\n","  \n","        pos_data_targets = positive_data_targets[np.random.permutation(positive_length), :]\n","        neg_data_targets = negative_data_targets[np.random.permutation(negative_length), :]\n","        \n","        if dv != 0:\n","            dv_positive = int(dv * positive_length/(positive_length + negative_length))\n","            dv_negative = dv - dv_positive\n","            dv_data_targets = np.concatenate((positive_data_targets[0:dv_positive,:], negative_data_targets[0:dv_negative,:]))\n","            dv_data_targets = dv_data_targets[np.random.permutation(dv), :]\n","            self.dv_targets = dv_data_targets[:,1].astype(float)\n","            X_dev = self.__tokenize__(dv_data_targets[:,0])\n","            self.X_dv_lengths = np.array(list(map(len, X_dev)))\n","            if words_ids == None:\n","                self.dv_data = X_dev\n","            else:\n","                self.dv_data = self.__ids_and_pad__(X_dev,words_ids[0], words_ids[1])\n","        else:\n","            dv_positive = 0\n","            dv_negative = 0\n","            \n","        if cv != 0:\n","            cv_positive = int(cv * positive_length/(positive_length + negative_length))\n","            cv_negative = cv - cv_positive\n","            cv_data_targets = np.concatenate((positive_data_targets[dv_positive:dv_positive + cv_positive,:], negative_data_targets[dv_negative:dv_negative + cv_negative,:]))\n","            cv_data_targets = cv_data_targets[np.random.permutation(cv), :]\n","            self.cv_targets = cv_data_targets[:,1].astype(float)\n","            X_cross = self.__tokenize__(cv_data_targets[:,0])\n","            self.X_cv_lengths = np.array(list(map(len, X_cross)))\n","            if words_ids == None:\n","                self.cv_data = X_cross\n","            else:\n","                self.cv_data = self.__ids_and_pad__(X_cross,words_ids[0], words_ids[1])   \n","        else:\n","            cv_positive = 0\n","            cv_negative = 0\n","            \n","        train_data_targets = np.concatenate((positive_data_targets[dv_positive + cv_positive:,:], negative_data_targets[dv_negative + cv_negative:,:]))\n","        train_data_targets = train_data_targets[np.random.permutation(len(data_targets)-dv-cv), :]\n","        self.train_targets = train_data_targets[:,1].astype(float)\n","        X_train = self.__tokenize__(train_data_targets[:,0])\n","        self.X_train_lengths = np.array(list(map(len, X_train)))\n","        if words_ids == None:\n","            self.train_data = X_train\n","        else:\n","            self.train_data = self.__ids_and_pad__(X_train ,words_ids[0], words_ids[1])\n","        self.num_train_examples = len(self.train_targets)\n","        self.nr_batches = self.num_train_examples//self.batch_length\n","    \n","    def __tokenize__(self, X):\n","        return [re.findall(r\"[\\w]+|[']|[.,!?;]\", str(x)) for x in X]\n","    \n","    def __ids_and_pad__(self, X, word_ids, width):\n","        padded_X = np.zeros((len(X),width))\n","        i = 0\n","        for sentence in X:\n","            j = 0\n","            for word in sentence:\n","                padded_X[i,j] = word_ids[word]\n","                j += 1\n","            i +=1\n","        return padded_X\n","    \n","    def cross_val_set(self):\n","        return self.X_cv_lengths, self.cv_data, self.cv_targets \n","    \n","    def dev_set(self):\n","        return self.X_dv_lengths, self.dv_data, self.dv_targets\n","    \n","    def next_batch(self):\n","        self.pointer += 1\n","        X_batch = self.train_data[(self.pointer - 1)*self.batch_length:self.pointer*self.batch_length]\n","        y_batch = self.train_targets[(self.pointer - 1)*self.batch_length:self.pointer*self.batch_length]\n","        X_batch_lengths = self.X_train_lengths[(self.pointer - 1)*self.batch_length:self.pointer*self.batch_length]\n","        \n","        if self.pointer == self.nr_batches:\n","            self.pointer = 0\n","            self.reshuffle_seed += 1\n","            np.random.seed(self.seed + self.reshuffle_seed)\n","            permutations = np.random.permutation(self.num_train_examples)\n","            self.train_data = self.train_data[permutations]\n","            self.train_targets = self.train_targets[permutations]\n","            self.X_train_lengths = self.X_train_lengths[permutations]\n","            \n","        return X_batch_lengths, X_batch, y_batch\n","    \n","    def reset(self):\n","        self.pointer = 0        "],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"8OJMLwdPV757"},"cell_type":"markdown","source":["# Load and prepare data"]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1545327174722,"user_tz":-120,"elapsed":48068,"user":{"displayName":"Quentin Legrand","photoUrl":"https://lh6.googleusercontent.com/-IXHiOGs04sE/AAAAAAAAAAI/AAAAAAAAAFk/ppsEvqzCcGw/s64/photo.jpg","userId":"00421493000358847389"}},"id":"06xzg79TV75-","outputId":"754d5fdd-97a5-4941-e680-f794ec7c2b2b","colab":{"base_uri":"https://localhost:8080/","height":206}},"cell_type":"code","source":["train_data = load_data()\n","train_data.head()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>qid</th>\n","      <th>target</th>\n","      <th>corrected_question_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>00002165364db923c7e6</td>\n","      <td>0</td>\n","      <td>How did Quebec nationalists see their province...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000032939017120e6e44</td>\n","      <td>0</td>\n","      <td>Do you have an adopted dog , how would you enc...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0000412ca6e4628ce2cf</td>\n","      <td>0</td>\n","      <td>Why does velocity affect time ? Does velocity ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>000042bf85aa498cd78e</td>\n","      <td>0</td>\n","      <td>How did Otto von Guericke used the Magdeburg h...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0000455dfa3e01eae3af</td>\n","      <td>0</td>\n","      <td>Can I convert montra helicon D to a mountain b...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                    qid  target  \\\n","0  00002165364db923c7e6       0   \n","1  000032939017120e6e44       0   \n","2  0000412ca6e4628ce2cf       0   \n","3  000042bf85aa498cd78e       0   \n","4  0000455dfa3e01eae3af       0   \n","\n","                             corrected_question_text  \n","0  How did Quebec nationalists see their province...  \n","1  Do you have an adopted dog , how would you enc...  \n","2  Why does velocity affect time ? Does velocity ...  \n","3  How did Otto von Guericke used the Magdeburg h...  \n","4  Can I convert montra helicon D to a mountain b...  "]},"metadata":{"tags":[]},"execution_count":6}]},{"metadata":{"colab_type":"code","id":"TFTmNHrfV76L","colab":{}},"cell_type":"code","source":["data_targets = df_to_data_target(train_data)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"s8-E1GHuV76V","colab":{}},"cell_type":"code","source":["tokenized_questions = tokenize(train_data[\"corrected_question_text\"].values)\n","vocabulary_in_set = get_vocab(tokenized_questions)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"31VolUA5V76l","colab":{}},"cell_type":"code","source":["embeddings, words_ids = glove_embeddings(vocabulary_in_set=vocabulary_in_set)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"pOVK2aYgV77U"},"cell_type":"markdown","source":["# Construction phase"]},{"metadata":{"colab_type":"code","id":"er4v1HWSV77a","colab":{"base_uri":"https://localhost:8080/","height":1166},"outputId":"4f1f28a8-9241-4f4a-c96f-ce76b3123f71","executionInfo":{"status":"error","timestamp":1545327431250,"user_tz":-120,"elapsed":304582,"user":{"displayName":"Quentin Legrand","photoUrl":"https://lh6.googleusercontent.com/-IXHiOGs04sE/AAAAAAAAAAI/AAAAAAAAAFk/ppsEvqzCcGw/s64/photo.jpg","userId":"00421493000358847389"}}},"cell_type":"code","source":["reset_graph()\n","\n","n_steps = 250\n","window_width = [3, 4, 5]\n","out_channels = [n_steps-window_width[0]+1, n_steps-window_width[1]+1, n_steps-window_width[2]+1]\n","embedding_dimensions = 300\n","n_fc1 = 300\n","n_fc2 = 300\n","n_fc3 = 300\n","n_fc4 = 200\n","n_fc5 = 100\n","n_output = 1\n","threshold = 1.\n","dropout_rate = 0.\n","momentum = 0.9\n","\n","activation_fn = tf.nn.elu\n","init_W_CL_3 = np.random.randn(window_width[0],embedding_dimensions,1,out_channels[0]).astype(np.float32)\n","init_b_CL_3 = np.random.randn(1,1,1,out_channels[0]).astype(np.float32)\n","init_W_CL_4 = np.random.randn(window_width[1],embedding_dimensions,1,out_channels[1]).astype(np.float32)\n","init_b_CL_4 = np.random.randn(1,1,1,out_channels[1]).astype(np.float32)\n","init_W_CL_5 = np.random.randn(window_width[2],embedding_dimensions,1,out_channels[2]).astype(np.float32)\n","init_b_CL_5 = np.random.randn(1,1,1,out_channels[2]).astype(np.float32)\n","\n","learning_rate = tf.placeholder(tf.float32)\n","\n","X = tf.placeholder(tf.int32, [None, n_steps], name=\"X\") #m * n_steps\n","y = tf.placeholder(tf.float32, [None], name=\"y\")\n","training = tf.placeholder_with_default(False, shape=(), name='training')\n","\n","with tf.name_scope(\"embedding_layer\"):\n","    word_embeddings = tf.Variable(initial_value=embeddings, trainable=False)# n_words * embedding_dimensions\n","    embedded_sentences = tf.nn.embedding_lookup(word_embeddings, X) # m * n_steps * embedding_dimensions\n","\n","with tf.name_scope(\"CNN_3\"):\n","    W_CL_3 = tf.Variable(initial_value=init_W_CL_3)\n","    b_CL_3 = tf.Variable(initial_value=init_b_CL_3)\n","    CL_3 = tf.nn.conv2d(tf.reshape(embedded_sentences,[-1,n_steps,embedding_dimensions,1]), W_CL_3, strides=[1,1,1,1], padding=\"VALID\")# m * n_steps - 3 + 1 *1 * out_channels \n","    activated_CL_3 = activation_fn(tf.add(CL_3,b_CL_3)) # m * n_steps - 5 + 1 * 1 * out_channels\n","    MAXPOOL_3 = tf.reshape(tf.math.reduce_max(activated_CL_3,axis=1),[-1,out_channels[0]]) # m * 1 * out_channels  --> m * out_channels\n","    \n","with tf.name_scope(\"CNN_4\"):\n","    W_CL_4 = tf.Variable(initial_value=init_W_CL_4)\n","    b_CL_4 = tf.Variable(initial_value=init_b_CL_4)\n","    CL_4 = tf.nn.conv2d(tf.reshape(embedded_sentences,[-1,n_steps,embedding_dimensions,1]), W_CL_4, strides=[1,1,1,1], padding=\"VALID\")# m * n_steps - 4 + 1 *1 * out_channels \n","    activated_CL_4 = activation_fn(tf.add(CL_4,b_CL_4)) # m * n_steps - 5 + 1 * 1 * out_channels\n","    MAXPOOL_4 = tf.reshape(tf.math.reduce_max(activated_CL_4,axis=1),[-1,out_channels[1]]) # m * 1 * out_channels  --> m * out_channels\n","    \n","with tf.name_scope(\"CNN_5\"):\n","    W_CL_5 = tf.Variable(initial_value=init_W_CL_5)\n","    b_CL_5 = tf.Variable(initial_value=init_b_CL_5)\n","    CL_5 = tf.nn.conv2d(tf.reshape(embedded_sentences,[-1,n_steps,embedding_dimensions,1]), W_CL_5, strides=[1,1,1,1], padding=\"VALID\")# m * n_steps - 5 + 1 *1 * out_channels \n","    activated_CL_5 = activation_fn(tf.add(CL_5,b_CL_5)) # m * n_steps - 5 + 1 * 1 * out_channels\n","    MAXPOOL_5 = tf.reshape(tf.math.reduce_max(activated_CL_5,axis=1),[-1,out_channels[2]]) # m * 1 * out_channels  --> m * out_channels\n","\n","with tf.name_scope(\"Max_pool\"):\n","    MAXPOOL = tf.concat([MAXPOOL_3,MAXPOOL_4,MAXPOOL_5], axis=1)\n","    \n","with tf.name_scope(\"fully_connected_layers\"):\n","    fc1 = tf.contrib.layers.fully_connected(MAXPOOL, n_fc1, activation_fn=activation_fn)\n","    fc2 = tf.contrib.layers.fully_connected(fc1, n_fc2, activation_fn=activation_fn)\n","    fc3 = tf.contrib.layers.fully_connected(fc2, n_fc3, activation_fn=activation_fn)\n","    fc4 = tf.contrib.layers.fully_connected(fc3, n_fc4, activation_fn=activation_fn)\n","    fc5 = tf.contrib.layers.fully_connected(fc4, n_fc5, activation_fn=activation_fn)\n","\n","\n","with tf.name_scope(\"logits_and_outputs\"):\n","    logits = tf.reshape(tf.contrib.layers.fully_connected(fc5, n_output, activation_fn=None),shape=[-1])\n","    outputs = tf.sigmoid(logits)\n","    predictions = tf.round(outputs)\n","\n","with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy)\n","\n","with tf.name_scope(\"evaluation\"):\n","    TP = tf.count_nonzero(predictions * y)\n","    TN = tf.count_nonzero((predictions - 1) * (y - 1))\n","    FP = tf.count_nonzero(predictions * (y - 1))\n","    FN = tf.count_nonzero((predictions - 1) * y)\n","    precision = TP / (TP + FP)\n","    recall = TP / (TP + FN)\n","    accuracy = (TP + TN)/(TP + FP + TN + FN)\n","    f1 = 2 * precision * recall / (precision + recall)\n","    f1_min = 1-f1\n","\n","with tf.name_scope(\"training_op\"):\n","    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","    grads_and_vars = optimizer.compute_gradients(f1_min)\n","    capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n","    training_op = optimizer.apply_gradients(capped_gvs)\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()\n","\n","f1_train_summary = tf.summary.scalar('Train_F1_score', f1)\n","f1_dev_summary = tf.summary.scalar(\"Dev_F1_score\", f1)\n","loss_train_summary = tf.summary.scalar(\"Train_loss\", loss)\n","loss_dev_summary = tf.summary.scalar(\"Dev_loss\", loss)"],"execution_count":10,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-9e691842f0fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mcapped_gvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mtraining_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapped_gvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-9e691842f0fc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mcapped_gvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mtraining_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapped_gvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/clip_ops.py\u001b[0m in \u001b[0;36mclip_by_value\u001b[0;34m(t, clip_value_min, clip_value_max, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m   with ops.name_scope(name, \"clip_by_value\",\n\u001b[1;32m     66\u001b[0m                       [t, clip_value_min, clip_value_max]) as name:\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Go through list of tensors, for each value in each tensor clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    227\u001b[0m                                          as_ref=False):\n\u001b[1;32m    228\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    206\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    207\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 208\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    209\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m   const_tensor = g.create_op(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    428\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None values not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m     \u001b[0;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;31m# provided if possible.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: None values not supported."]}]},{"metadata":{"colab_type":"text","id":"-66y9w6WV77g"},"cell_type":"markdown","source":["# Training phase"]},{"metadata":{"colab_type":"code","id":"MayLFImnV77p","colab":{}},"cell_type":"code","source":["batch_size = 500\n","\n","QIQC = data_split_and_batch(data_targets, cv=1000, dv=1000, batch_size=batch_size, words_ids=(words_ids,n_steps))\n","X_dev_lengths, X_dev_set, y_dev_set = QIQC.dev_set()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7xp-fEfZoxD-","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"lengths, x_b, y_b = QIQC.next_batch()\n","item = x_b[26]\n","sentence = []\n","for i in range(lengths[26]):\n","  for word, key in words_ids.items():\n","    if item[i] == key:\n","      sentence.append(word)\n","sentence, y_b[26]\"\"\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"_Pd99HmxEvwl","colab_type":"code","colab":{}},"cell_type":"code","source":["model_version = \"4.0.3\""],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"U6YJBLgtV775","colab":{}},"cell_type":"code","source":["QIQC.reset()\n","n_epoch = 8\n","now = dt.utcnow().strftime(\"%Y%m%d%H%M%S\")\n","LOGS_FOLDER = \"logs/run-{}/\".format(now)\n","MODELS_FILE = \"models/model{}/QIQC_model{}.ckpt\".format(model_version,model_version)\n","logdir = os.path.join(DRIVE_PATH,LOGS_FOLDER)\n","modfil = os.path.join(DRIVE_PATH,MODELS_FILE)\n","\n","#file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n","\n","t_init = time.time()\n","lr_stop = 1\n","lr_stack = []\n","loss_stack = []\n","step_stack = []\n","f1_stack = []\n","loss_train_stack = []\n","f1_train_stack = []\n","\n","lr = 0.004\n","\n","with tf.Session() as sess:\n","    init.run()\n","    t_0 = time.time()\n","    for epoch in range(n_epoch):\n","        if epoch % 2 == 0 and epoch != 0 :\n","          lr = lr/2\n","        for iteration in range(QIQC.nr_batches):            \n","            X_lengths, X_batch, y_batch = QIQC.next_batch()\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, learning_rate: lr, training: True})\n","            step = epoch * QIQC.nr_batches + iteration\n","            if step % 20 ==0:\n","              f1_dev, loss_dev = sess.run([f1, loss], feed_dict = {X : X_dev_set, y : y_dev_set})\n","              f1_train, loss_train = sess.run([f1, loss], feed_dict = {X : X_batch, y : y_batch})\n","              step_stack.append(step)\n","              loss_stack.append(loss_dev)\n","              f1_stack.append(f1_dev)\n","              loss_train_stack.append(loss_train)\n","              f1_train_stack.append(f1_train)\n","            \"\"\"\n","            if iteration % 30 == 0 :\n","                f1_train_str, loss_train_str = sess.run([f1_train_summary, loss_train_summary],feed_dict = {X : X_batch, y : y_batch, seq_len: X_lengths})\n","                f1_dev_str, loss_dev_str = sess.run([f1_dev_summary, loss_dev_summary], feed_dict = {X : X_dev_set, y : y_dev_set, seq_len: X_dev_lengths})\n","                file_writer.add_summary(f1_train_str, step)\n","                file_writer.add_summary(f1_dev_str, step)\n","                file_writer.add_summary(loss_train_str, step)\n","                file_writer.add_summary(loss_dev_str, step)\n","            \"\"\"\n","            if iteration % 1000 == 0:\n","                save_path = saver.save(sess,modfil)\n","            if iteration % 100 == 0:\n","                #f1_train, loss_train = sess.run([f1, loss],feed_dict = {X : X_batch, y : y_batch})\n","                #f1_dev, loss_dev = sess.run([f1, loss], feed_dict = {X : X_dev_set, y : y_dev_set})\n","                t_1 = time.time()\n","                t_delta = t_1 - t_0\n","                expected_runtime = t_delta * ((n_epoch - epoch - 1) * QIQC.nr_batches + (QIQC.nr_batches - iteration))/100\n","                expected_runtime_h = int(expected_runtime//3600)\n","                expected_runtime_min = int((expected_runtime - 3600 * expected_runtime_h)//60)\n","                expected_runtime_sec = int((expected_runtime - 3600 * expected_runtime_h - 60 * expected_runtime_min))\n","                t_0 = t_1\n","                print(\"Ep. {}/{}; It. {}/{}; Runtime {:.2f} s; Remaining runtime {}h{}min{}s; f1_train {:.4f}; f1_dev {:.4f}; loss_train {:.4f}; loss_dev {:.4f}\".format(epoch + 1, n_epoch, iteration, QIQC.nr_batches, t_delta, expected_runtime_h,expected_runtime_min,expected_runtime_sec,f1_train,f1_dev,loss_train,loss_dev))\n","    save_path = saver.save(sess,modfil)\n","t_final = time.time()\n","actual_runtime_h = int((t_final - t_init)//3600)\n","actual_runtime_min = int((t_final - t_init - 3600 * actual_runtime_h)//60)\n","actual_runtime_sec = int((t_final - t_init - 3600 * actual_runtime_h - 60 * actual_runtime_min))\n","\n","print(\"Total actual runtime: {} hours {} min {} sec\".format(actual_runtime_h, actual_runtime_min, actual_runtime_sec))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HpvXmcPrPgOX","colab_type":"code","colab":{}},"cell_type":"code","source":["alpha = 0.1\n","ewma_loss_dev = numpy_ewma_vectorized(loss_stack, alpha)\n","ewma_f1_dev = numpy_ewma_vectorized(f1_stack, alpha)\n","ewma_loss_train = numpy_ewma_vectorized(loss_train_stack, alpha)\n","ewma_f1_train = numpy_ewma_vectorized(f1_train_stack, alpha)\n","\n","plt.figure(figsize = (20, 16))\n","plt.subplot(221) \n","plt.plot(step_stack, ewma_loss_dev)\n","plt.xlabel(\"Learning Rate\")\n","plt.ylabel(\"Loss dev\")\n","#plt.xlim([0.001,0.008])\n","plt.ylim([0,.3])\n","plt.subplot(222)\n","plt.plot(step_stack, ewma_f1_dev)\n","#plt.xlim([0.001,0.008])\n","#plt.ylim([0.55,0.75])\n","plt.xlabel(\"Learning Rate\")\n","plt.ylabel(\"F1 dev\")\n","\n","plt.subplot(223) \n","plt.plot(step_stack, ewma_loss_train)\n","plt.xlabel(\"Learning Rate\")\n","plt.ylabel(\"Loss Train\")\n","#plt.xlim([0.001,0.008])\n","plt.ylim([0,0.3])\n","plt.subplot(224)\n","plt.plot(step_stack, ewma_f1_train)\n","#plt.xlim([0.001,0.008])\n","#plt.ylim([0.55,0.75])\n","plt.xlabel(\"Learning Rate\")\n","plt.ylabel(\"F1 Train\")\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5fAfqgZZyAtW","colab_type":"code","colab":{}},"cell_type":"code","source":["MODELS_FILE = \"models/model{}/QIQC_model{}.ckpt\".format(model_version,model_version)\n","modfil = os.path.join(DRIVE_PATH,MODELS_FILE)\n","X_cv_lengths, X_cv_set, y_cv_set = QIQC.cross_val_set()\n","\n","with tf.Session() as sess: \n","    saver.restore(sess, modfil)\n","    f1_cv = f1.eval(feed_dict = {X : X_cv_set, y : y_cv_set})\n","    f1_dev = f1.eval(feed_dict = {X : X_dev_set, y : y_dev_set})\n","\n","f1_cv_pc = f1_cv * 100\n","f1_dev_pc = f1_dev * 100\n","print(\"cross val f1 {:.2f}%, dev f1 {:.2f}%\".format(f1_cv_pc, f1_dev_pc))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9mk0AYC9zxlu","colab_type":"code","colab":{}},"cell_type":"code","source":["print(modfil)"],"execution_count":0,"outputs":[]}]}